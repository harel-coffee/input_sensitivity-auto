{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2S\n",
    "\n",
    ">@inproceedings{jamshidi2018,\n",
    "    title={Learning to sample: exploiting similarities across environments to learn performance models for configurable systems}, \n",
    "    author={Jamshidi, Pooyan and Velez, Miguel and K{\\\"a}stner, Christian and Siegmund, Norbert},\n",
    "    booktitle={Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},\n",
    "    pages={71--82},\n",
    "    year={2018},\n",
    "    organization={ACM},\n",
    "    url={https://dl.acm.org/doi/pdf/10.1145/3236024.3236074},\n",
    "}\n",
    "\n",
    "**Learning to Sample (L2S)** is a transfer learning approach defined by Jamshidi et al. \n",
    "First, it exploits the source input and selects configurations that leverage influential (interactions of) features for this input. \n",
    "Then, it explores the similarities between the source and the target, thus adding configurations having similar performances for the source and the target. \n",
    "Finally, it uses the configurations selected in previous steps to efficiently train a model on the target input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for arrays\n",
    "import numpy as np\n",
    "\n",
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# high-level plots\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# machine learning library\n",
    "# Principal Component Analysis - determine new axis for representing data\n",
    "from sklearn.decomposition import PCA\n",
    "# Random Forests -> vote between decision trees\n",
    "# Gradient boosting -> instead of a vote, upgrade the same tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "# To add interactions in linear regressions models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Elasticnet is an hybrid method between ridge and Lasso\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# To separate the data into training and test\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Simple clustering (iterative steps)\n",
    "from sklearn.cluster import KMeans\n",
    "# Support vector machine - support vector regressor\n",
    "from sklearn.svm import SVR\n",
    "# decision trees\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# gradient boosting trees\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# we use it to interact with the file system\n",
    "import os\n",
    "# compute time\n",
    "from time import time\n",
    "\n",
    "# Neural network high level framework\n",
    "import keras\n",
    "# Sequential is a sequence of blocs\n",
    "# Input deals with the data fed to the network\n",
    "from keras.models import Sequential,Input,Model\n",
    "# Dense is a feedforward layer with fully connected nodes\n",
    "# Dropout allows to keep part of data, and to \"drop out\" a the rest\n",
    "# Flatten makes the data \"flat\", i.e. in one dimension\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "# Conv -> convolution, MaxPooling is relative to Pooling\n",
    "# Activation if the function composing the data in output of a layer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "\n",
    "\n",
    "from learner.mlearner import learn_with_interactions, learn_without_interactions, sample_random, stepwise_feature_selection\n",
    "from learner.model import genModelTermsfromString, Model, genModelfromCoeff\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implementation of the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class L2S:\n",
    "    \n",
    "    def __init__(self):\n",
    "        #self.pct_test = pct_test\n",
    "        #self.ratio_exploitation = ratio_exploitation\n",
    "        \n",
    "        # the data folder, see the markdown there for additional explanations\n",
    "        res_dir = \"../../../data/ugc/res_ugc/\"\n",
    "        \n",
    "        # the list of videos names, e.g. Animation_360P-3e40\n",
    "        # we sort the list so we keep the same ids between two launches\n",
    "        v_names = sorted(os.listdir(res_dir)) \n",
    "\n",
    "        self.predDimension = \"kbs\"\n",
    "        \n",
    "        # the list of measurements\n",
    "        listVideo = []\n",
    "\n",
    "        # we add each dataset in the list, converting the time to the right format\n",
    "        # third line asserts that the measures are complete\n",
    "        for v in v_names:\n",
    "            data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "            inter = pd.get_dummies(data)\n",
    "            inter[self.predDimension] = data[self.predDimension]\n",
    "            listVideo.append(inter)\n",
    "        \n",
    "        self.listVideo = listVideo\n",
    "        \n",
    "        \n",
    "        # to sample the source and the target using the same seed\n",
    "        self.random_state = np.random.randint(0,1000)\n",
    "        \n",
    "        self.features = ['cabac', '8x8dct', 'mbtree', 'rc_lookahead', 'analyse', 'me', 'subme', 'mixed_ref', 'me_range', \n",
    "                 'qpmax', 'aq-mode', 'trellis','fast_pskip', 'chroma_qp_offset', 'bframes', 'b_pyramid', \n",
    "                 'b_adapt', 'direct', 'ref', 'deblock', 'weightb', 'open_gop', 'weightp', 'scenecut']\n",
    "    \n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true-y_pred)**2)\n",
    "    \n",
    "    ### Step 1: Extraction Process of Performance Models\n",
    "    \n",
    "    #Select a good model for predicting the performance of the source video\n",
    "    \n",
    "    #Original files:\n",
    "    #https://github.com/cmu-mars/model-learner/blob/tutorial/learner/mlearner.py for the stepwise selection\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html for the interactions\n",
    "    \n",
    "    # @PooyanJamshidi:\n",
    "    # We just change slightly some functions from the original repository,\n",
    "    # mainly because we don't want to add a constant in the model\n",
    "    # + steps 2 and 3 were implemented in matlab but we did not find them in python\n",
    "    def stepwise_selection(self, X, y,\n",
    "                           initial_list = [], \n",
    "                           threshold_in = 0.01, \n",
    "                           threshold_out = 0.05, \n",
    "                           verbose=False):\n",
    "\n",
    "        ndim = X.shape[1]\n",
    "        features = [i for i in range(ndim)]\n",
    "        included = list(initial_list)\n",
    "\n",
    "        while True:\n",
    "            changed=False\n",
    "\n",
    "            # forward step (removed a constant)\n",
    "            excluded = list(set(features)-set(included))\n",
    "            new_pval = pd.Series(index=excluded)\n",
    "            for new_column in excluded:\n",
    "                model = sm.OLS(y, pd.DataFrame(X[included+[new_column]])).fit()\n",
    "                new_pval[new_column] = model.pvalues[new_column]\n",
    "            best_pval = new_pval.min()\n",
    "            if best_pval < threshold_in:\n",
    "                best_feature = new_pval.idxmin()\n",
    "                included.append(best_feature)\n",
    "                changed=True\n",
    "                if verbose:\n",
    "                    print('Add {:30} with p-value {:.5}'.format(best_feature, best_pval))\n",
    "\n",
    "            # backward step\n",
    "            model = sm.OLS(y, pd.DataFrame(X[included])).fit()\n",
    "            pvalues = model.pvalues\n",
    "            worst_pval = pvalues.max()\n",
    "            if worst_pval > threshold_out:\n",
    "                changed = True\n",
    "                worst_feature = pvalues.idxmax()\n",
    "                included.remove(worst_feature)\n",
    "                if verbose:\n",
    "                    print('Drop {:30} with p-value {:.5}'.format(worst_feature, worst_pval))\n",
    "            if not changed:\n",
    "                if verbose:\n",
    "                    print(\"Construction of the model completed!\")\n",
    "                break\n",
    "        return included\n",
    "    \n",
    "    ### Step 2: Active Sampling\n",
    "    \n",
    "    #### A - ] Exploitation : use the source's prediction model\n",
    "    \n",
    "    ##### (i) Sort the coefficients of the previous constructed model\n",
    "    ##### (ii) Choose the coefficient with the highest value\n",
    "    ##### (iii) Select the configurations with this feature activated\n",
    "\n",
    "    # I assumed it was recursive, with a decreasing influence in the selection \n",
    "    # for a decreasing importance in the regression.\n",
    "    \n",
    "    def select_exploitation(self, df, sc, config_selected):\n",
    "        \n",
    "        self.nb_config = int(self.nb_config_exploitation - len(config_selected))\n",
    "        \n",
    "        if self.nb_config == 0:\n",
    "            #print(\"Done!\\n\")\n",
    "            return config_selected\n",
    "\n",
    "        # if we don't have any important coefficient left to help us choose configs\n",
    "        # we take the nb_config first configurations\n",
    "        if len(sc) == 0:\n",
    "            #print(\"Selecting \" + str(self.nb_config) + \" configurations from the rest of the dataset!\")\n",
    "            for conf in df.index[0:self.nb_config]:\n",
    "                config_selected.append(conf)\n",
    "            return config_selected\n",
    "\n",
    "        # otherwise we just use the best coef to choose configs\n",
    "        else:\n",
    "\n",
    "            # we choose the best features coef (biggest absolute value)\n",
    "            most_important_coef = sc[0]\n",
    "\n",
    "            #print(\"Feature : \" + str(most_important_coef))\n",
    "\n",
    "            # configs with this feature activated\n",
    "            imp_index = np.where(df[most_important_coef]==1)[0]\n",
    "\n",
    "            # number of configs with this feature activated\n",
    "            nb_imp_index = len(imp_index)\n",
    "\n",
    "            # if we have more values to choose \n",
    "            # than the number of configurations with the best feature activated\n",
    "            # we add all the configuration to the selected set\n",
    "            # and we select the rest of the configuration based on other coefficients\n",
    "            if nb_imp_index <= self.nb_config:\n",
    "                for conf in df.iloc[imp_index].index:\n",
    "                    config_selected.append(conf)\n",
    "                #if nb_imp_index > 0:\n",
    "                #    print(\"Added \"+str(nb_imp_index)+ \" values, \"+\n",
    "                #          str(self.nb_config-nb_imp_index)+\" left to choose \\n\")\n",
    "                # then we apply recursively this method to the rest of the dataframe\n",
    "                return self.select_exploitation(df.iloc[np.where(df[most_important_coef]==0)[0]], \n",
    "                                              sc[1:len(sc)],\n",
    "                                              config_selected)\n",
    "\n",
    "            # otherwise we have enough values with this features activated\n",
    "            # to select all the remaining configurations\n",
    "            # so we apply the method to the dataframe containing all the feature activated\n",
    "            # and we select the configuration by using the followings features\n",
    "            else:\n",
    "                return self.select_exploitation(df.iloc[imp_index], \n",
    "                                     sc[1:len(sc)], \n",
    "                                     config_selected)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### B-] Exploration : Select specific configurations, similar between the source and the target\n",
    "    \n",
    "    # I choose to select the group in one step:\n",
    "    # if you select config per config, you may choose a local optimal\n",
    "    \n",
    "    def select_exploration(self, exploitation_conf, ratio_exploitation, number_group = 10):\n",
    "        \n",
    "        nb_exploration = int(np.round(self.config_tot*(1-ratio_exploitation)))\n",
    "\n",
    "        #target = self.listVideo[id_target]\n",
    "\n",
    "        # all the config left for exploration\n",
    "        # total minus those chosen for exploitation\n",
    "        explor_conf = np.setdiff1d(self.source.index, exploitation_conf)\n",
    "\n",
    "        # initialization : we take the first nb_exploration config\n",
    "        best_explor = explor_conf[0:nb_exploration]\n",
    "\n",
    "        # we group it with the exploitation configurations\n",
    "        conf = np.concatenate((exploitation_conf, best_explor), axis=0)\n",
    "        \n",
    "        # for the moment, it's our best entropy\n",
    "        best_entropy = sc.entropy(self.target.iloc[conf][self.predDimension], \n",
    "                                  self.source.iloc[conf][self.predDimension])\n",
    "\n",
    "        # then we incrementally select the configurations to diminish the entropy \n",
    "        group_counter = 0\n",
    "\n",
    "        while group_counter < number_group:\n",
    "\n",
    "            group_counter +=1\n",
    "\n",
    "            # current group to 'challenge' the best result\n",
    "            np.random.shuffle(explor_conf)\n",
    "            current_explor = explor_conf[0:nb_exploration]\n",
    "\n",
    "            # we group it with the exploitation configurations\n",
    "            conf = np.concatenate((exploitation_conf, current_explor), axis=0)\n",
    "\n",
    "            # we compute the Kullback Leibler divergence between the source and the target\n",
    "            current_entropy = sc.entropy(self.target.iloc[conf][self.predDimension], \n",
    "                                         self.source.iloc[conf][self.predDimension])\n",
    "\n",
    "            # we finally take the group giving the lowest entropy\n",
    "            # if this group is better than the best group, we replace it by the new one\n",
    "            if current_entropy > best_entropy:\n",
    "                #print(\"Entropy gained : \"+str(current_entropy-best_entropy))\n",
    "                best_entropy = current_entropy\n",
    "                best_explor = current_explor\n",
    "\n",
    "        return best_explor\n",
    "    \n",
    "    \n",
    "    \n",
    "    def learn(self, source_id, target_id, ratio_exploitation = 0.3, \n",
    "              l2s_tr_ratio = 0.8, \n",
    "              train_size = 20,\n",
    "              learning_algorithm = RandomForestRegressor):\n",
    "\n",
    "        # the source video\n",
    "        self.source = self.listVideo[source_id]\n",
    "\n",
    "        # the number of config used in the training\n",
    "        self.config_tot = int(train_size)\n",
    "        \n",
    "        if train_size <= 1:\n",
    "            self.config_tot = int(train_size*self.source.shape[1])\n",
    "\n",
    "        # transform some variables into dummies, to fit the orginal paper\n",
    "        # since we don't want to introduce a meaningless constant in the model, \n",
    "        # we have to keep all columns\n",
    "\n",
    "        X_src = pd.DataFrame(np.array(self.source.drop([self.predDimension], axis = 1), dtype=int))\n",
    "\n",
    "        #X_src = self.source[self.keep_features]\n",
    "        \n",
    "        # add interactions\n",
    "        poly = PolynomialFeatures(degree = 1, interaction_only = True, include_bias = True)\n",
    "        \n",
    "        # degree 2 take too much time + it will not scale for large configuration spaces...\n",
    "        # IMO O(n) or O(nlog(n)) are the complexity we should all target for our algorithms\n",
    "        \n",
    "        X_interact = pd.DataFrame(np.array(poly.fit_transform(X_src), int))\n",
    "\n",
    "        # performance variable, to predict\n",
    "        y_src = self.source[self.predDimension]\n",
    "        \n",
    "        # we train the model with the training data\n",
    "        \n",
    "        # print(\"\\n############### I- Knowledge extraction #################\\n\")\n",
    "\n",
    "        selected_features = self.stepwise_selection(X_interact, y_src)\n",
    "\n",
    "        # print(\"\\n############### II- Sampling #################\\n\")\n",
    "\n",
    "        reg = LinearRegression()\n",
    "\n",
    "        reg.fit(X_interact[selected_features], y_src)\n",
    "\n",
    "        sorted_coefs = pd.Series(np.abs(reg.coef_), \n",
    "                                 selected_features, \n",
    "                                 dtype='float64').sort_values(ascending=False).index\n",
    "\n",
    "        # print(\"A- EXPLOITATION\\n\")\n",
    "        \n",
    "        self.nb_config_exploitation = int(ratio_exploitation*self.config_tot)\n",
    "        \n",
    "        exploitation_conf = self.select_exploitation(X_interact, sorted_coefs, [])\n",
    "        \n",
    "        # print(exploitation_conf)\n",
    "\n",
    "        # print(\"\\nB- EXPLORATION\\n\")\n",
    "\n",
    "        # we ensure we sample the configurations of the training set\n",
    "        # which removes the potential threat of using the configuration of the testing set\n",
    "        # during the training\n",
    "        \n",
    "        # target\n",
    "        self.target = self.listVideo[target_id]\n",
    "        \n",
    "        exploration_conf = self.select_exploration(exploitation_conf, ratio_exploitation)\n",
    "\n",
    "        sampled_conf = np.concatenate((exploitation_conf,exploration_conf), axis=0)\n",
    "        \n",
    "        # print(sampled_conf)\n",
    "        \n",
    "        # print(\"\\n############### III- Performance Model Learning #################\\n\")\n",
    "\n",
    "        # we build a performance model for the target\n",
    "        # instead of using all the configurations, we use the sampled configuration\n",
    "        # ie we remove the unnecessary configurations\n",
    "        # print(len(sampled_conf))\n",
    "        \n",
    "        X_tgt = self.target.drop([self.predDimension], axis = 1)\n",
    "        y_tgt = self.target[self.predDimension]\n",
    "\n",
    "        X_tgt_tr = X_tgt.iloc[sampled_conf]\n",
    "        y_tgt_tr = y_tgt[sampled_conf]\n",
    "        \n",
    "        #X_tgt_te = self.target[self.keep_features].drop(sampled_conf, inplace = False, axis=0)\n",
    "        #y_tgt_te = self.target[self.predDimension].drop(sampled_conf, inplace = False, axis=0)\n",
    "\n",
    "        # The shift function, to transfer the prediction from the source to the target\n",
    "        lf = learning_algorithm()\n",
    "        lf.fit(X_tgt_tr, y_tgt_tr)\n",
    "        y_tgt_pred = lf.predict(X_tgt)\n",
    "        \n",
    "        # We return the mean average percentage error \n",
    "        # between the real values of y_test from target \n",
    "        # and the predictions shifted \n",
    "        return self.mse(y_tgt, y_tgt_pred)\n",
    "    \n",
    "    def predict_conf(self, source_id, target_id, ratio_exploitation = 0.3, \n",
    "              l2s_tr_ratio = 0.8, \n",
    "              train_size = 20,\n",
    "              learning_algorithm = XGBRegressor):\n",
    "\n",
    "        # the source video\n",
    "        self.source = self.listVideo[source_id]\n",
    "\n",
    "        # the number of config used in the training\n",
    "        self.config_tot = int(train_size)\n",
    "        \n",
    "        if train_size <= 1:\n",
    "            self.config_tot = int(train_size*self.source.shape[1])\n",
    "\n",
    "        # transform some variables into dummies, to fit the orginal paper\n",
    "        # since we don't want to introduce a meaningless constant in the model, \n",
    "        # we have to keep all columns\n",
    "\n",
    "        X_src = pd.DataFrame(np.array(self.source.drop([self.predDimension], axis = 1), dtype=int))\n",
    "\n",
    "        #X_src = self.source[self.keep_features]\n",
    "        \n",
    "        # add interactions\n",
    "        poly = PolynomialFeatures(degree = 1, interaction_only = True, include_bias = True)\n",
    "        \n",
    "        # degree 2 take too much time + it will not scale for large configuration spaces...\n",
    "        # IMO O(n) or O(nlog(n)) are the complexity we should all target for our algorithms\n",
    "        \n",
    "        X_interact = pd.DataFrame(np.array(poly.fit_transform(X_src), int))\n",
    "\n",
    "        # performance variable, to predict\n",
    "        y_src = self.source[self.predDimension]\n",
    "        \n",
    "        # we train the model with the training data\n",
    "        \n",
    "        # print(\"\\n############### I- Knowledge extraction #################\\n\")\n",
    "\n",
    "        selected_features = self.stepwise_selection(X_interact, y_src)\n",
    "\n",
    "        # print(\"\\n############### II- Sampling #################\\n\")\n",
    "\n",
    "        reg = LinearRegression()\n",
    "\n",
    "        reg.fit(X_interact[selected_features], y_src)\n",
    "\n",
    "        sorted_coefs = pd.Series(np.abs(reg.coef_), \n",
    "                                 selected_features, \n",
    "                                 dtype='float64').sort_values(ascending=False).index\n",
    "\n",
    "        # print(\"A- EXPLOITATION\\n\")\n",
    "        \n",
    "        self.nb_config_exploitation = int(ratio_exploitation*self.config_tot)\n",
    "        \n",
    "        exploitation_conf = self.select_exploitation(X_interact, sorted_coefs, [])\n",
    "        \n",
    "        # print(exploitation_conf)\n",
    "\n",
    "        # print(\"\\nB- EXPLORATION\\n\")\n",
    "\n",
    "        # we ensure we sample the configurations of the training set\n",
    "        # which removes the potential threat of using the configuration of the testing set\n",
    "        # during the training\n",
    "        \n",
    "        # target\n",
    "        self.target = self.listVideo[target_id]\n",
    "        \n",
    "        exploration_conf = self.select_exploration(exploitation_conf, ratio_exploitation)\n",
    "\n",
    "        sampled_conf = np.concatenate((exploitation_conf,exploration_conf), axis=0)\n",
    "        \n",
    "        #print(\"\\n############### III- Performance Model Learning #################\\n\")\n",
    "\n",
    "        # we build a performance model for the target\n",
    "        # instead of using all the configurations, we use the sampled configuration\n",
    "        # ie we remove the unnecessary configurations\n",
    "        # print(len(sampled_conf))\n",
    "        \n",
    "        X_tgt = self.target.drop([self.predDimension], axis = 1)\n",
    "        y_tgt = self.target[self.predDimension]\n",
    "\n",
    "        X_tgt_tr = X_tgt.iloc[sampled_conf]\n",
    "        y_tgt_tr = y_tgt[sampled_conf]\n",
    "        \n",
    "        #X_tgt_te = self.target[self.keep_features].drop(sampled_conf, inplace = False, axis=0)\n",
    "        #y_tgt_te = self.target[self.predDimension].drop(sampled_conf, inplace = False, axis=0)\n",
    "\n",
    "        # The shift function, to transfer the prediction from the source to the target\n",
    "        lf = learning_algorithm()\n",
    "        lf.fit(X_tgt_tr, y_tgt_tr)\n",
    "        y_tgt_pred = lf.predict(X_tgt)\n",
    "        \n",
    "        # We return the mean average percentage error \n",
    "        # between the real values of y_test from target \n",
    "        # and the predictions shifted \n",
    "        return np.argmin(y_tgt_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> 3281685.2859895527\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> 2042005.232342937\n",
      "<class 'xgboost.sklearn.XGBRegressor'> 3062540.786840061\n",
      "<class 'sklearn.svm._classes.SVR'> 9576159.427733865\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> 27405524.146231342\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> 6519916.737222477\n",
      "<class 'xgboost.sklearn.XGBRegressor'> 7263449.448959386\n",
      "<class 'sklearn.svm._classes.SVR'> 71663105.14875856\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> 10962.400734825867\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> 6764.398971992097\n",
      "<class 'xgboost.sklearn.XGBRegressor'> 2998.9533698724767\n",
      "<class 'sklearn.svm._classes.SVR'> 61580.4388545609\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> 1613.7505716417907\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> 2706.459407840895\n",
      "<class 'xgboost.sklearn.XGBRegressor'> 5054.611589117543\n",
      "<class 'sklearn.svm._classes.SVR'> 16379.99248684554\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'> 1783955.5832855713\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'> 1267925.4245734932\n",
      "<class 'xgboost.sklearn.XGBRegressor'> 618574.6346787198\n",
      "<class 'sklearn.svm._classes.SVR'> 6327715.289539618\n"
     ]
    }
   ],
   "source": [
    "l2s = L2S()\n",
    "\n",
    "LAs = [DecisionTreeRegressor, RandomForestRegressor, XGBRegressor, SVR]\n",
    "for i in range(5):\n",
    "    source_id = np.random.randint(0,1000)\n",
    "    target_id = np.random.randint(0,1000)\n",
    "    for la in LAs:\n",
    "        print(la, l2s.learn(source_id = source_id, \n",
    "                            target_id = target_id, \n",
    "                            train_size = 20, \n",
    "                            learning_algorithm=la))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen algorithm :  XGBRegressor (however it may depends on the choice of videos)\n",
    "\n",
    "Bug with Linear Regression, mse too low? To analyze\n",
    "\n",
    "Again, it depends on the video we consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "\n",
    "We predict the configurations for each video of the test set, for 5 configs, 10 configs, ..., 30 configs in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data folder, see the markdown there for additional explanations\n",
    "res_dir = \"../../../data/ugc/res_ugc/\"\n",
    "\n",
    "# the list of videos names, e.g. Animation_360P-3e40\n",
    "# we sort the list so we keep the same ids between two launches\n",
    "v_names = sorted(os.listdir(res_dir)) \n",
    "\n",
    "v_names_train = np.loadtxt(\"../../../results/raw_data/train_names.csv\", dtype= str)\n",
    "v_names_test = np.loadtxt(\"../../../results/raw_data/test_names.csv\", dtype= str)\n",
    "index_train = [i for i in range(len(v_names)) if v_names[i] in v_names_train]\n",
    "index_test = [i for i in range(len(v_names)) if v_names[i] in v_names_test]\n",
    "\n",
    "train_sizes = np.arange(5,31,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2s = L2S()\n",
    "l2s_confs = dict()\n",
    "for i in range(len(index_test)):\n",
    "    it = index_test[i]\n",
    "    source_index_train = np.random.randint(0, len(v_names_train))\n",
    "    source_id = index_train[source_index_train]\n",
    "    for ts in train_sizes:\n",
    "        l2s_confs[(i, ts)] = l2s.predict_conf(source_id = source_id, target_id = it, train_size=ts,\n",
    "                                      learning_algorithm = XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 5): 1,\n",
       " (0, 10): 34,\n",
       " (0, 15): 153,\n",
       " (0, 20): 123,\n",
       " (0, 25): 166,\n",
       " (0, 30): 123,\n",
       " (1, 5): 57,\n",
       " (1, 10): 169,\n",
       " (1, 15): 93,\n",
       " (1, 20): 189,\n",
       " (1, 25): 100,\n",
       " (1, 30): 89,\n",
       " (2, 5): 111,\n",
       " (2, 10): 163,\n",
       " (2, 15): 176,\n",
       " (2, 20): 190,\n",
       " (2, 25): 190,\n",
       " (2, 30): 161,\n",
       " (3, 5): 85,\n",
       " (3, 10): 4,\n",
       " (3, 15): 32,\n",
       " (3, 20): 161,\n",
       " (3, 25): 194,\n",
       " (3, 30): 155,\n",
       " (4, 5): 76,\n",
       " (4, 10): 91,\n",
       " (4, 15): 10,\n",
       " (4, 20): 175,\n",
       " (4, 25): 42,\n",
       " (4, 30): 79,\n",
       " (5, 5): 8,\n",
       " (5, 10): 87,\n",
       " (5, 15): 103,\n",
       " (5, 20): 100,\n",
       " (5, 25): 97,\n",
       " (5, 30): 189,\n",
       " (6, 5): 49,\n",
       " (6, 10): 168,\n",
       " (6, 15): 166,\n",
       " (6, 20): 100,\n",
       " (6, 25): 159,\n",
       " (6, 30): 168,\n",
       " (7, 5): 48,\n",
       " (7, 10): 189,\n",
       " (7, 15): 3,\n",
       " (7, 20): 98,\n",
       " (7, 25): 92,\n",
       " (7, 30): 92,\n",
       " (8, 5): 19,\n",
       " (8, 10): 85,\n",
       " (8, 15): 181,\n",
       " (8, 20): 170,\n",
       " (8, 25): 179,\n",
       " (8, 30): 181,\n",
       " (9, 5): 0,\n",
       " (9, 10): 62,\n",
       " (9, 15): 178,\n",
       " (9, 20): 96,\n",
       " (9, 25): 109,\n",
       " (9, 30): 189,\n",
       " (10, 5): 8,\n",
       " (10, 10): 62,\n",
       " (10, 15): 123,\n",
       " (10, 20): 91,\n",
       " (10, 25): 91,\n",
       " (10, 30): 96,\n",
       " (11, 5): 4,\n",
       " (11, 10): 178,\n",
       " (11, 15): 123,\n",
       " (11, 20): 169,\n",
       " (11, 25): 165,\n",
       " (11, 30): 197,\n",
       " (12, 5): 92,\n",
       " (12, 10): 100,\n",
       " (12, 15): 85,\n",
       " (12, 20): 96,\n",
       " (12, 25): 123,\n",
       " (12, 30): 189,\n",
       " (13, 5): 29,\n",
       " (13, 10): 74,\n",
       " (13, 15): 62,\n",
       " (13, 20): 171,\n",
       " (13, 25): 200,\n",
       " (13, 30): 171,\n",
       " (14, 5): 6,\n",
       " (14, 10): 168,\n",
       " (14, 15): 137,\n",
       " (14, 20): 153,\n",
       " (14, 25): 184,\n",
       " (14, 30): 154,\n",
       " (15, 5): 1,\n",
       " (15, 10): 103,\n",
       " (15, 15): 87,\n",
       " (15, 20): 75,\n",
       " (15, 25): 192,\n",
       " (15, 30): 65,\n",
       " (16, 5): 1,\n",
       " (16, 10): 78,\n",
       " (16, 15): 169,\n",
       " (16, 20): 109,\n",
       " (16, 25): 93,\n",
       " (16, 30): 101,\n",
       " (17, 5): 166,\n",
       " (17, 10): 168,\n",
       " (17, 15): 166,\n",
       " (17, 20): 166,\n",
       " (17, 25): 157,\n",
       " (17, 30): 168,\n",
       " (18, 5): 20,\n",
       " (18, 10): 83,\n",
       " (18, 15): 25,\n",
       " (18, 20): 123,\n",
       " (18, 25): 192,\n",
       " (18, 30): 169,\n",
       " (19, 5): 4,\n",
       " (19, 10): 63,\n",
       " (19, 15): 189,\n",
       " (19, 20): 85,\n",
       " (19, 25): 97,\n",
       " (19, 30): 90,\n",
       " (20, 5): 8,\n",
       " (20, 10): 83,\n",
       " (20, 15): 169,\n",
       " (20, 20): 89,\n",
       " (20, 25): 91,\n",
       " (20, 30): 168,\n",
       " (21, 5): 123,\n",
       " (21, 10): 156,\n",
       " (21, 15): 100,\n",
       " (21, 20): 94,\n",
       " (21, 25): 85,\n",
       " (21, 30): 170,\n",
       " (22, 5): 97,\n",
       " (22, 10): 181,\n",
       " (22, 15): 20,\n",
       " (22, 20): 67,\n",
       " (22, 25): 196,\n",
       " (22, 30): 21,\n",
       " (23, 5): 159,\n",
       " (23, 10): 64,\n",
       " (23, 15): 168,\n",
       " (23, 20): 200,\n",
       " (23, 25): 100,\n",
       " (23, 30): 106,\n",
       " (24, 5): 4,\n",
       " (24, 10): 190,\n",
       " (24, 15): 85,\n",
       " (24, 20): 169,\n",
       " (24, 25): 38,\n",
       " (24, 30): 104,\n",
       " (25, 5): 12,\n",
       " (25, 10): 172,\n",
       " (25, 15): 176,\n",
       " (25, 20): 104,\n",
       " (25, 25): 32,\n",
       " (25, 30): 171,\n",
       " (26, 5): 4,\n",
       " (26, 10): 79,\n",
       " (26, 15): 98,\n",
       " (26, 20): 85,\n",
       " (26, 25): 112,\n",
       " (26, 30): 92,\n",
       " (27, 5): 0,\n",
       " (27, 10): 54,\n",
       " (27, 15): 102,\n",
       " (27, 20): 100,\n",
       " (27, 25): 48,\n",
       " (27, 30): 176,\n",
       " (28, 5): 49,\n",
       " (28, 10): 182,\n",
       " (28, 15): 105,\n",
       " (28, 20): 165,\n",
       " (28, 25): 169,\n",
       " (28, 30): 184,\n",
       " (29, 5): 123,\n",
       " (29, 10): 17,\n",
       " (29, 15): 101,\n",
       " (29, 20): 196,\n",
       " (29, 25): 108,\n",
       " (29, 30): 108,\n",
       " (30, 5): 35,\n",
       " (30, 10): 20,\n",
       " (30, 15): 177,\n",
       " (30, 20): 159,\n",
       " (30, 25): 123,\n",
       " (30, 30): 183,\n",
       " (31, 5): 1,\n",
       " (31, 10): 164,\n",
       " (31, 15): 189,\n",
       " (31, 20): 102,\n",
       " (31, 25): 45,\n",
       " (31, 30): 112,\n",
       " (32, 5): 32,\n",
       " (32, 10): 101,\n",
       " (32, 15): 132,\n",
       " (32, 20): 56,\n",
       " (32, 25): 104,\n",
       " (32, 30): 46,\n",
       " (33, 5): 4,\n",
       " (33, 10): 80,\n",
       " (33, 15): 36,\n",
       " (33, 20): 68,\n",
       " (33, 25): 176,\n",
       " (33, 30): 187,\n",
       " (34, 5): 20,\n",
       " (34, 10): 80,\n",
       " (34, 15): 104,\n",
       " (34, 20): 104,\n",
       " (34, 25): 96,\n",
       " (34, 30): 104,\n",
       " (35, 5): 143,\n",
       " (35, 10): 119,\n",
       " (35, 15): 42,\n",
       " (35, 20): 47,\n",
       " (35, 25): 47,\n",
       " (35, 30): 113,\n",
       " (36, 5): 123,\n",
       " (36, 10): 159,\n",
       " (36, 15): 169,\n",
       " (36, 20): 181,\n",
       " (36, 25): 184,\n",
       " (36, 30): 177,\n",
       " (37, 5): 134,\n",
       " (37, 10): 70,\n",
       " (37, 15): 112,\n",
       " (37, 20): 175,\n",
       " (37, 25): 92,\n",
       " (37, 30): 23,\n",
       " (38, 5): 1,\n",
       " (38, 10): 70,\n",
       " (38, 15): 112,\n",
       " (38, 20): 51,\n",
       " (38, 25): 42,\n",
       " (38, 30): 23,\n",
       " (39, 5): 65,\n",
       " (39, 10): 153,\n",
       " (39, 15): 153,\n",
       " (39, 20): 170,\n",
       " (39, 25): 168,\n",
       " (39, 30): 168,\n",
       " (40, 5): 74,\n",
       " (40, 10): 153,\n",
       " (40, 15): 165,\n",
       " (40, 20): 98,\n",
       " (40, 25): 43,\n",
       " (40, 30): 112,\n",
       " (41, 5): 69,\n",
       " (41, 10): 42,\n",
       " (41, 15): 168,\n",
       " (41, 20): 21,\n",
       " (41, 25): 193,\n",
       " (41, 30): 193,\n",
       " (42, 5): 168,\n",
       " (42, 10): 178,\n",
       " (42, 15): 91,\n",
       " (42, 20): 93,\n",
       " (42, 25): 183,\n",
       " (42, 30): 91,\n",
       " (43, 5): 85,\n",
       " (43, 10): 159,\n",
       " (43, 15): 86,\n",
       " (43, 20): 107,\n",
       " (43, 25): 86,\n",
       " (43, 30): 91,\n",
       " (44, 5): 157,\n",
       " (44, 10): 3,\n",
       " (44, 15): 23,\n",
       " (44, 20): 154,\n",
       " (44, 25): 170,\n",
       " (44, 30): 168,\n",
       " (45, 5): 123,\n",
       " (45, 10): 177,\n",
       " (45, 15): 171,\n",
       " (45, 20): 169,\n",
       " (45, 25): 181,\n",
       " (45, 30): 179,\n",
       " (46, 5): 153,\n",
       " (46, 10): 181,\n",
       " (46, 15): 91,\n",
       " (46, 20): 171,\n",
       " (46, 25): 169,\n",
       " (46, 30): 184,\n",
       " (47, 5): 79,\n",
       " (47, 10): 89,\n",
       " (47, 15): 25,\n",
       " (47, 20): 165,\n",
       " (47, 25): 159,\n",
       " (47, 30): 159,\n",
       " (48, 5): 119,\n",
       " (48, 10): 54,\n",
       " (48, 15): 161,\n",
       " (48, 20): 27,\n",
       " (48, 25): 177,\n",
       " (48, 30): 173,\n",
       " (49, 5): 1,\n",
       " (49, 10): 61,\n",
       " (49, 15): 47,\n",
       " (49, 20): 92,\n",
       " (49, 25): 2,\n",
       " (49, 30): 28,\n",
       " (50, 5): 178,\n",
       " (50, 10): 164,\n",
       " (50, 15): 91,\n",
       " (50, 20): 10,\n",
       " (50, 25): 43,\n",
       " (50, 30): 196,\n",
       " (51, 5): 46,\n",
       " (51, 10): 171,\n",
       " (51, 15): 130,\n",
       " (51, 20): 111,\n",
       " (51, 25): 130,\n",
       " (51, 30): 131,\n",
       " (52, 5): 0,\n",
       " (52, 10): 183,\n",
       " (52, 15): 35,\n",
       " (52, 20): 168,\n",
       " (52, 25): 104,\n",
       " (52, 30): 159,\n",
       " (53, 5): 87,\n",
       " (53, 10): 12,\n",
       " (53, 15): 85,\n",
       " (53, 20): 107,\n",
       " (53, 25): 104,\n",
       " (53, 30): 177,\n",
       " (54, 5): 25,\n",
       " (54, 10): 113,\n",
       " (54, 15): 113,\n",
       " (54, 20): 23,\n",
       " (54, 25): 113,\n",
       " (54, 30): 92,\n",
       " (55, 5): 155,\n",
       " (55, 10): 155,\n",
       " (55, 15): 111,\n",
       " (55, 20): 32,\n",
       " (55, 25): 104,\n",
       " (55, 30): 34,\n",
       " (56, 5): 74,\n",
       " (56, 10): 181,\n",
       " (56, 15): 137,\n",
       " (56, 20): 74,\n",
       " (56, 25): 170,\n",
       " (56, 30): 159,\n",
       " (57, 5): 4,\n",
       " (57, 10): 168,\n",
       " (57, 15): 105,\n",
       " (57, 20): 102,\n",
       " (57, 25): 95,\n",
       " (57, 30): 105,\n",
       " (58, 5): 189,\n",
       " (58, 10): 181,\n",
       " (58, 15): 89,\n",
       " (58, 20): 172,\n",
       " (58, 25): 183,\n",
       " (58, 30): 96,\n",
       " (59, 5): 148,\n",
       " (59, 10): 171,\n",
       " (59, 15): 160,\n",
       " (59, 20): 60,\n",
       " (59, 25): 60,\n",
       " (59, 30): 32,\n",
       " (60, 5): 165,\n",
       " (60, 10): 157,\n",
       " (60, 15): 182,\n",
       " (60, 20): 182,\n",
       " (60, 25): 169,\n",
       " (60, 30): 170,\n",
       " (61, 5): 74,\n",
       " (61, 10): 171,\n",
       " (61, 15): 123,\n",
       " (61, 20): 104,\n",
       " (61, 25): 35,\n",
       " (61, 30): 34,\n",
       " (62, 5): 1,\n",
       " (62, 10): 197,\n",
       " (62, 15): 195,\n",
       " (62, 20): 166,\n",
       " (62, 25): 159,\n",
       " (62, 30): 123,\n",
       " (63, 5): 29,\n",
       " (63, 10): 56,\n",
       " (63, 15): 56,\n",
       " (63, 20): 104,\n",
       " (63, 25): 34,\n",
       " (63, 30): 131,\n",
       " (64, 5): 20,\n",
       " (64, 10): 189,\n",
       " (64, 15): 68,\n",
       " (64, 20): 32,\n",
       " (64, 25): 190,\n",
       " (64, 30): 104,\n",
       " (65, 5): 166,\n",
       " (65, 10): 105,\n",
       " (65, 15): 178,\n",
       " (65, 20): 104,\n",
       " (65, 25): 20,\n",
       " (65, 30): 189,\n",
       " (66, 5): 189,\n",
       " (66, 10): 54,\n",
       " (66, 15): 35,\n",
       " (66, 20): 134,\n",
       " (66, 25): 32,\n",
       " (66, 30): 131,\n",
       " (67, 5): 64,\n",
       " (67, 10): 112,\n",
       " (67, 15): 175,\n",
       " (67, 20): 123,\n",
       " (67, 25): 10,\n",
       " (67, 30): 193,\n",
       " (68, 5): 111,\n",
       " (68, 10): 91,\n",
       " (68, 15): 91,\n",
       " (68, 20): 165,\n",
       " (68, 25): 74,\n",
       " (68, 30): 170,\n",
       " (69, 5): 0,\n",
       " (69, 10): 166,\n",
       " (69, 15): 175,\n",
       " (69, 20): 197,\n",
       " (69, 25): 123,\n",
       " (69, 30): 159,\n",
       " (70, 5): 1,\n",
       " (70, 10): 42,\n",
       " (70, 15): 169,\n",
       " (70, 20): 91,\n",
       " (70, 25): 85,\n",
       " (70, 30): 91,\n",
       " (71, 5): 46,\n",
       " (71, 10): 165,\n",
       " (71, 15): 189,\n",
       " (71, 20): 46,\n",
       " (71, 25): 68,\n",
       " (71, 30): 104,\n",
       " (72, 5): 8,\n",
       " (72, 10): 20,\n",
       " (72, 15): 197,\n",
       " (72, 20): 93,\n",
       " (72, 25): 173,\n",
       " (72, 30): 100,\n",
       " (73, 5): 1,\n",
       " (73, 10): 176,\n",
       " (73, 15): 9,\n",
       " (73, 20): 168,\n",
       " (73, 25): 27,\n",
       " (73, 30): 28,\n",
       " (74, 5): 99,\n",
       " (74, 10): 85,\n",
       " (74, 15): 170,\n",
       " (74, 20): 100,\n",
       " (74, 25): 75,\n",
       " (74, 30): 107,\n",
       " (75, 5): 61,\n",
       " (75, 10): 2,\n",
       " (75, 15): 82,\n",
       " (75, 20): 42,\n",
       " (75, 25): 92,\n",
       " (75, 30): 3,\n",
       " (76, 5): 23,\n",
       " (76, 10): 25,\n",
       " (76, 15): 25,\n",
       " (76, 20): 169,\n",
       " (76, 25): 20,\n",
       " (76, 30): 24,\n",
       " (77, 5): 70,\n",
       " (77, 10): 168,\n",
       " (77, 15): 104,\n",
       " (77, 20): 190,\n",
       " (77, 25): 171,\n",
       " (77, 30): 46,\n",
       " (78, 5): 0,\n",
       " (78, 10): 149,\n",
       " (78, 15): 81,\n",
       " (78, 20): 60,\n",
       " (78, 25): 46,\n",
       " (78, 30): 176,\n",
       " (79, 5): 194,\n",
       " (79, 10): 68,\n",
       " (79, 15): 160,\n",
       " (79, 20): 26,\n",
       " (79, 25): 56,\n",
       " (79, 30): 60,\n",
       " (80, 5): 119,\n",
       " (80, 10): 165,\n",
       " (80, 15): 91,\n",
       " (80, 20): 20,\n",
       " (80, 25): 159,\n",
       " (80, 30): 169,\n",
       " (81, 5): 1,\n",
       " (81, 10): 153,\n",
       " (81, 15): 159,\n",
       " (81, 20): 26,\n",
       " (81, 25): 153,\n",
       " (81, 30): 158,\n",
       " (82, 5): 1,\n",
       " (82, 10): 123,\n",
       " (82, 15): 100,\n",
       " (82, 20): 183,\n",
       " (82, 25): 91,\n",
       " (82, 30): 166,\n",
       " (83, 5): 4,\n",
       " (83, 10): 177,\n",
       " (83, 15): 169,\n",
       " (83, 20): 159,\n",
       " (83, 25): 192,\n",
       " (83, 30): 158,\n",
       " (84, 5): 130,\n",
       " (84, 10): 38,\n",
       " (84, 15): 60,\n",
       " (84, 20): 132,\n",
       " (84, 25): 160,\n",
       " (84, 30): 171,\n",
       " (85, 5): 111,\n",
       " (85, 10): 32,\n",
       " (85, 15): 85,\n",
       " (85, 20): 26,\n",
       " (85, 25): 104,\n",
       " (85, 30): 189,\n",
       " (86, 5): 145,\n",
       " (86, 10): 130,\n",
       " (86, 15): 101,\n",
       " (86, 20): 104,\n",
       " (86, 25): 190,\n",
       " (86, 30): 32,\n",
       " (87, 5): 1,\n",
       " (87, 10): 160,\n",
       " (87, 15): 46,\n",
       " (87, 20): 188,\n",
       " (87, 25): 171,\n",
       " (87, 30): 32,\n",
       " (88, 5): 38,\n",
       " (88, 10): 155,\n",
       " (88, 15): 176,\n",
       " (88, 20): 199,\n",
       " (88, 25): 81,\n",
       " (88, 30): 81,\n",
       " (89, 5): 134,\n",
       " (89, 10): 200,\n",
       " (89, 15): 100,\n",
       " (89, 20): 170,\n",
       " (89, 25): 176,\n",
       " (89, 30): 129,\n",
       " (90, 5): 49,\n",
       " (90, 10): 98,\n",
       " (90, 15): 91,\n",
       " (90, 20): 15,\n",
       " (90, 25): 75,\n",
       " (90, 30): 105,\n",
       " (91, 5): 39,\n",
       " (91, 10): 74,\n",
       " (91, 15): 91,\n",
       " (91, 20): 91,\n",
       " (91, 25): 189,\n",
       " (91, 30): 169,\n",
       " (92, 5): 91,\n",
       " (92, 10): 166,\n",
       " (92, 15): 78,\n",
       " (92, 20): 47,\n",
       " (92, 25): 193,\n",
       " (92, 30): 175,\n",
       " (93, 5): 91,\n",
       " (93, 10): 85,\n",
       " (93, 15): 62,\n",
       " (93, 20): 25,\n",
       " (93, 25): 101,\n",
       " (93, 30): 170,\n",
       " (94, 5): 1,\n",
       " (94, 10): 42,\n",
       " (94, 15): 179,\n",
       " (94, 20): 169,\n",
       " (94, 25): 181,\n",
       " (94, 30): 181,\n",
       " (95, 5): 20,\n",
       " (95, 10): 154,\n",
       " (95, 15): 20,\n",
       " (95, 20): 169,\n",
       " (95, 25): 169,\n",
       " (95, 30): 173,\n",
       " (96, 5): 2,\n",
       " (96, 10): 3,\n",
       " (96, 15): 164,\n",
       " (96, 20): 175,\n",
       " (96, 25): 112,\n",
       " (96, 30): 47,\n",
       " (97, 5): 83,\n",
       " (97, 10): 170,\n",
       " (97, 15): 79,\n",
       " (97, 20): 42,\n",
       " (97, 25): 42,\n",
       " (97, 30): 112,\n",
       " (98, 5): 107,\n",
       " (98, 10): 47,\n",
       " (98, 15): 106,\n",
       " (98, 20): 175,\n",
       " (98, 25): 42,\n",
       " (98, 30): 123,\n",
       " (99, 5): 99,\n",
       " (99, 10): 123,\n",
       " (99, 15): 104,\n",
       " (99, 20): 176,\n",
       " (99, 25): 46,\n",
       " (99, 30): 187,\n",
       " (100, 5): 159,\n",
       " (100, 10): 166,\n",
       " (100, 15): 134,\n",
       " (100, 20): 45,\n",
       " (100, 25): 165,\n",
       " (100, 30): 112,\n",
       " (101, 5): 29,\n",
       " (101, 10): 153,\n",
       " (101, 15): 3,\n",
       " (101, 20): 141,\n",
       " (101, 25): 2,\n",
       " (101, 30): 159,\n",
       " (102, 5): 1,\n",
       " (102, 10): 169,\n",
       " (102, 15): 93,\n",
       " (102, 20): 104,\n",
       " (102, 25): 171,\n",
       " (102, 30): 170,\n",
       " (103, 5): 58,\n",
       " (103, 10): 153,\n",
       " (103, 15): 84,\n",
       " (103, 20): 185,\n",
       " (103, 25): 123,\n",
       " (103, 30): 165,\n",
       " (104, 5): 85,\n",
       " (104, 10): 164,\n",
       " (104, 15): 43,\n",
       " (104, 20): 112,\n",
       " (104, 25): 170,\n",
       " (104, 30): 164,\n",
       " (105, 5): 153,\n",
       " (105, 10): 179,\n",
       " (105, 15): 93,\n",
       " (105, 20): 185,\n",
       " (105, 25): 168,\n",
       " (105, 30): 169,\n",
       " (106, 5): 143,\n",
       " (106, 10): 20,\n",
       " (106, 15): 159,\n",
       " (106, 20): 94,\n",
       " (106, 25): 89,\n",
       " (106, 30): 166,\n",
       " (107, 5): 71,\n",
       " (107, 10): 39,\n",
       " (107, 15): 123,\n",
       " (107, 20): 47,\n",
       " (107, 25): 39,\n",
       " (107, 30): 112,\n",
       " (108, 5): 179,\n",
       " (108, 10): 159,\n",
       " (108, 15): 169,\n",
       " (108, 20): 169,\n",
       " (108, 25): 154,\n",
       " (108, 30): 159,\n",
       " (109, 5): 176,\n",
       " (109, 10): 46,\n",
       " (109, 15): 187,\n",
       " (109, 20): 187,\n",
       " (109, 25): 187,\n",
       " (109, 30): 48,\n",
       " (110, 5): 1,\n",
       " (110, 10): 1,\n",
       " (110, 15): 83,\n",
       " (110, 20): 170,\n",
       " (110, 25): 200,\n",
       " (110, 30): 180,\n",
       " (111, 5): 35,\n",
       " (111, 10): 85,\n",
       " (111, 15): 45,\n",
       " (111, 20): 2,\n",
       " (111, 25): 193,\n",
       " (111, 30): 112,\n",
       " (112, 5): 62,\n",
       " (112, 10): 65,\n",
       " (112, 15): 90,\n",
       " (112, 20): 101,\n",
       " (112, 25): 189,\n",
       " (112, 30): 170,\n",
       " (113, 5): 20,\n",
       " (113, 10): 177,\n",
       " (113, 15): 159,\n",
       " (113, 20): 91,\n",
       " (113, 25): 159,\n",
       " (113, 30): 196,\n",
       " (114, 5): 62,\n",
       " (114, 10): 153,\n",
       " (114, 15): 3,\n",
       " (114, 20): 91,\n",
       " (114, 25): 168,\n",
       " (114, 30): 89,\n",
       " (115, 5): 42,\n",
       " (115, 10): 42,\n",
       " (115, 15): 175,\n",
       " (115, 20): 45,\n",
       " (115, 25): 92,\n",
       " (115, 30): 47,\n",
       " (116, 5): 99,\n",
       " (116, 10): 8,\n",
       " (116, 15): 130,\n",
       " (116, 20): 104,\n",
       " (116, 25): 32,\n",
       " (116, 30): 171,\n",
       " (117, 5): 6,\n",
       " (117, 10): 2,\n",
       " (117, 15): 47,\n",
       " (117, 20): 112,\n",
       " (117, 25): 47,\n",
       " (117, 30): 16,\n",
       " (118, 5): 123,\n",
       " (118, 10): 166,\n",
       " (118, 15): 94,\n",
       " (118, 20): 89,\n",
       " (118, 25): 183,\n",
       " (118, 30): 196,\n",
       " (119, 5): 134,\n",
       " (119, 10): 123,\n",
       " (119, 15): 75,\n",
       " (119, 20): 93,\n",
       " (119, 25): 75,\n",
       " (119, 30): 94,\n",
       " (120, 5): 20,\n",
       " (120, 10): 100,\n",
       " (120, 15): 8,\n",
       " (120, 20): 159,\n",
       " (120, 25): 179,\n",
       " (120, 30): 89,\n",
       " (121, 5): 137,\n",
       " (121, 10): 153,\n",
       " (121, 15): 3,\n",
       " (121, 20): 166,\n",
       " (121, 25): 169,\n",
       " (121, 30): 169,\n",
       " (122, 5): 42,\n",
       " (122, 10): 137,\n",
       " (122, 15): 169,\n",
       " (122, 20): 184,\n",
       " (122, 25): 159,\n",
       " (122, 30): 3,\n",
       " (123, 5): 80,\n",
       " (123, 10): 46,\n",
       " (123, 15): 101,\n",
       " (123, 20): 91,\n",
       " (123, 25): 103,\n",
       " (123, 30): 168,\n",
       " (124, 5): 65,\n",
       " (124, 10): 181,\n",
       " (124, 15): 168,\n",
       " (124, 20): 101,\n",
       " (124, 25): 179,\n",
       " (124, 30): 89,\n",
       " (125, 5): 157,\n",
       " (125, 10): 172,\n",
       " (125, 15): 172,\n",
       " (125, 20): 172,\n",
       " (125, 25): 165,\n",
       " (125, 30): 166,\n",
       " (126, 5): 87,\n",
       " (126, 10): 95,\n",
       " (126, 15): 91,\n",
       " (126, 20): 85,\n",
       " (126, 25): 189,\n",
       " (126, 30): 170,\n",
       " (127, 5): 153,\n",
       " (127, 10): 74,\n",
       " (127, 15): 12,\n",
       " (127, 20): 169,\n",
       " (127, 25): 54,\n",
       " (127, 30): 104,\n",
       " (128, 5): 92,\n",
       " (128, 10): 181,\n",
       " (128, 15): 94,\n",
       " (128, 20): 168,\n",
       " (128, 25): 159,\n",
       " (128, 30): 184,\n",
       " (129, 5): 1,\n",
       " (129, 10): 164,\n",
       " (129, 15): 123,\n",
       " (129, 20): 2,\n",
       " (129, 25): 195,\n",
       " (129, 30): 123,\n",
       " (130, 5): 151,\n",
       " (130, 10): 168,\n",
       " (130, 15): 91,\n",
       " (130, 20): 196,\n",
       " (130, 25): 96,\n",
       " (130, 30): 169,\n",
       " (131, 5): 11,\n",
       " (131, 10): 173,\n",
       " (131, 15): 192,\n",
       " (131, 20): 4,\n",
       " (131, 25): 104,\n",
       " (131, 30): 12,\n",
       " (132, 5): 46,\n",
       " (132, 10): 159,\n",
       " (132, 15): 169,\n",
       " (132, 20): 169,\n",
       " (132, 25): 159,\n",
       " (132, 30): 165,\n",
       " (133, 5): 123,\n",
       " (133, 10): 181,\n",
       " (133, 15): 3,\n",
       " (133, 20): 23,\n",
       " (133, 25): 112,\n",
       " (133, 30): 28,\n",
       " (134, 5): 52,\n",
       " (134, 10): 134,\n",
       " (134, 15): 25,\n",
       " (134, 20): 23,\n",
       " (134, 25): 16,\n",
       " (134, 30): 112,\n",
       " (135, 5): 168,\n",
       " (135, 10): 62,\n",
       " (135, 15): 91,\n",
       " (135, 20): 100,\n",
       " (135, 25): 85,\n",
       " (135, 30): 107,\n",
       " (136, 5): 20,\n",
       " (136, 10): 165,\n",
       " (136, 15): 75,\n",
       " (136, 20): 172,\n",
       " (136, 25): 170,\n",
       " (136, 30): 91,\n",
       " (137, 5): 2,\n",
       " (137, 10): 160,\n",
       " (137, 15): 160,\n",
       " (137, 20): 104,\n",
       " (137, 25): 60,\n",
       " (137, 30): 42,\n",
       " (138, 5): 48,\n",
       " (138, 10): 176,\n",
       " (138, 15): 131,\n",
       " (138, 20): 48,\n",
       " (138, 25): 160,\n",
       " (138, 30): 34,\n",
       " (139, 5): 34,\n",
       " (139, 10): 46,\n",
       " (139, 15): 190,\n",
       " (139, 20): 189,\n",
       " (139, 25): 91,\n",
       " (139, 30): 43,\n",
       " (140, 5): 57,\n",
       " (140, 10): 134,\n",
       " (140, 15): 25,\n",
       " (140, 20): 57,\n",
       " (140, 25): 60,\n",
       " (140, 30): 104,\n",
       " (141, 5): 120,\n",
       " (141, 10): 70,\n",
       " (141, 15): 47,\n",
       " (141, 20): 43,\n",
       " (141, 25): 47,\n",
       " (141, 30): 10,\n",
       " (142, 5): 32,\n",
       " (142, 10): 20,\n",
       " (142, 15): 179,\n",
       " (142, 20): 170,\n",
       " (142, 25): 185,\n",
       " (142, 30): 180,\n",
       " (143, 5): 39,\n",
       " (143, 10): 29,\n",
       " (143, 15): 153,\n",
       " (143, 20): 91,\n",
       " (143, 25): 153,\n",
       " (143, 30): 169,\n",
       " (144, 5): 74,\n",
       " (144, 10): 165,\n",
       " (144, 15): 196,\n",
       " (144, 20): 168,\n",
       " (144, 25): 177,\n",
       " (144, 30): 91,\n",
       " (145, 5): 0,\n",
       " (145, 10): 134,\n",
       " (145, 15): 179,\n",
       " (145, 20): 154,\n",
       " (145, 25): 169,\n",
       " (145, 30): 123,\n",
       " (146, 5): 86,\n",
       " (146, 10): 41,\n",
       " (146, 15): 165,\n",
       " (146, 20): 196,\n",
       " (146, 25): 189,\n",
       " (146, 30): 172,\n",
       " (147, 5): 90,\n",
       " (147, 10): 97,\n",
       " (147, 15): 109,\n",
       " (147, 20): 108,\n",
       " (147, 25): 101,\n",
       " (147, 30): 166,\n",
       " (148, 5): 20,\n",
       " (148, 10): 85,\n",
       " (148, 15): 100,\n",
       " (148, 20): 100,\n",
       " (148, 25): 3,\n",
       " (148, 30): 93,\n",
       " (149, 5): 0,\n",
       " (149, 10): 168,\n",
       " (149, 15): 16,\n",
       " (149, 20): 42,\n",
       " (149, 25): 169,\n",
       " (149, 30): 164,\n",
       " (150, 5): 85,\n",
       " (150, 10): 75,\n",
       " (150, 15): 97,\n",
       " (150, 20): 85,\n",
       " (150, 25): 85,\n",
       " (150, 30): 75,\n",
       " (151, 5): 1,\n",
       " (151, 10): 134,\n",
       " (151, 15): 191,\n",
       " (151, 20): 193,\n",
       " (151, 25): 175,\n",
       " (151, 30): 26,\n",
       " (152, 5): 181,\n",
       " (152, 10): 97,\n",
       " (152, 15): 50,\n",
       " (152, 20): 170,\n",
       " (152, 25): 74,\n",
       " (152, 30): 179,\n",
       " (153, 5): 20,\n",
       " (153, 10): 123,\n",
       " (153, 15): 160,\n",
       " (153, 20): 104,\n",
       " (153, 25): 171,\n",
       " (153, 30): 54,\n",
       " (154, 5): 99,\n",
       " (154, 10): 153,\n",
       " (154, 15): 25,\n",
       " (154, 20): 186,\n",
       " (154, 25): 165,\n",
       " (154, 30): 169,\n",
       " (155, 5): 111,\n",
       " (155, 10): 130,\n",
       " (155, 15): 136,\n",
       " (155, 20): 160,\n",
       " (155, 25): 46,\n",
       " (155, 30): 161,\n",
       " (156, 5): 91,\n",
       " (156, 10): 169,\n",
       " (156, 15): 159,\n",
       " (156, 20): 165,\n",
       " (156, 25): 183,\n",
       " (156, 30): 8,\n",
       " (157, 5): 1,\n",
       " (157, 10): 95,\n",
       " (157, 15): 85,\n",
       " (157, 20): 183,\n",
       " (157, 25): 3,\n",
       " (157, 30): 92,\n",
       " (158, 5): 8,\n",
       " (158, 10): 85,\n",
       " (158, 15): 95,\n",
       " (158, 20): 159,\n",
       " (158, 25): 181,\n",
       " (158, 30): 170,\n",
       " (159, 5): 181,\n",
       " (159, 10): 85,\n",
       " (159, 15): 184,\n",
       " (159, 20): 184,\n",
       " (159, 25): 123,\n",
       " (159, 30): 166,\n",
       " (160, 5): 140,\n",
       " (160, 10): 43,\n",
       " (160, 15): 158,\n",
       " (160, 20): 159,\n",
       " (160, 25): 182,\n",
       " (160, 30): 2,\n",
       " (161, 5): 8,\n",
       " (161, 10): 85,\n",
       " (161, 15): 62,\n",
       " (161, 20): 170,\n",
       " (161, 25): 85,\n",
       " (161, 30): 62,\n",
       " (162, 5): 181,\n",
       " (162, 10): 169,\n",
       " (162, 15): 165,\n",
       " (162, 20): 182,\n",
       " (162, 25): 158,\n",
       " (162, 30): 166,\n",
       " (163, 5): 168,\n",
       " (163, 10): 54,\n",
       " (163, 15): 170,\n",
       " (163, 20): 36,\n",
       " (163, 25): 171,\n",
       " (163, 30): 131,\n",
       " (164, 5): 0,\n",
       " (164, 10): 32,\n",
       " (164, 15): 161,\n",
       " (164, 20): 177,\n",
       " (164, 25): 164,\n",
       " (164, 30): 170,\n",
       " (165, 5): 159,\n",
       " (165, 10): 168,\n",
       " (165, 15): 166,\n",
       " (165, 20): 166,\n",
       " (165, 25): 166,\n",
       " (165, 30): 168,\n",
       " (166, 5): 132,\n",
       " (166, 10): 43,\n",
       " (166, 15): 92,\n",
       " (166, 20): 175,\n",
       " ...}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2s_confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2s_data = pd.DataFrame({\"id_video\" : [i for i in range(len(index_test))]})\n",
    "for ts in train_sizes:\n",
    "    l2s_data[\"conf\"+str(ts)] = [l2s_confs[(i, ts)] for i in range(len(index_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2s_data.set_index(\"id_video\").to_csv(\"../../../results/raw_data/L2S_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
