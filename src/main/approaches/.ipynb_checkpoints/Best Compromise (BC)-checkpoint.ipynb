{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best compromise (BC)\n",
    "\n",
    "**Best compromise (BC)** applies a performance model on all the training set, without making a difference between input videos. \n",
    "It selects the configuration working best for most videos in the training set. \n",
    "Technically, we rank the 201 configurations (1 being the optimal configuration, and 201 the worst) and select the one optimizing the sum of ranks for all input videos in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for arrays\n",
    "import numpy as np\n",
    "\n",
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "# high-level plots\n",
    "import seaborn as sns\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# machine learning library\n",
    "# Principal Component Analysis - determine new axis for representing data\n",
    "from sklearn.decomposition import PCA\n",
    "# Random Forests -> vote between decision trees\n",
    "# Gradient boosting -> instead of a vote, upgrade the same tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "# To add interactions in linear regressions models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Elasticnet is an hybrid method between ridge and Lasso\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# To separate the data into training and test\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Simple clustering (iterative steps)\n",
    "from sklearn.cluster import KMeans\n",
    "# Support vector machine - support vector regressor\n",
    "from sklearn.svm import SVR\n",
    "# decision trees\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# gradient boosting trees\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# we use it to interact with the file system\n",
    "import os\n",
    "# compute time\n",
    "from time import time\n",
    "\n",
    "# Neural network high level framework\n",
    "import keras\n",
    "# Sequential is a sequence of blocs\n",
    "# Input deals with the data fed to the network\n",
    "from keras.models import Sequential,Input,Model\n",
    "# Dense is a feedforward layer with fully connected nodes\n",
    "# Dropout allows to keep part of data, and to \"drop out\" a the rest\n",
    "# Flatten makes the data \"flat\", i.e. in one dimension\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "# Conv -> convolution, MaxPooling is relative to Pooling\n",
    "# Activation if the function composing the data in output of a layer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set of input videos - Join all the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_names_train = np.loadtxt(\"../../../results/raw_data/train_names.csv\", dtype= str)\n",
    "\n",
    "predDimension = 'kbs'\n",
    "\n",
    "#because x264 output is \"m:s\", where m is the number of minutes and s the number of seconds \n",
    "# we define a function to convert this format into the number of seconds\n",
    "def elapsedtime_to_sec(el):\n",
    "    tab = el.split(\":\")\n",
    "    return float(tab[0])*60+float(tab[1])\n",
    "\n",
    "# the data folder, see the markdown there for additional explanations\n",
    "res_dir = \"../../../data/ugc/res_ugc/\"\n",
    "\n",
    "# the list of videos names, e.g. Animation_360P-3e40\n",
    "# we sort the list so we keep the same ids between two launches\n",
    "v_names = sorted(os.listdir(res_dir)) \n",
    "\n",
    "to_dummy_features = [ 'rc_lookahead', 'analyse', 'me', 'subme', 'mixed_ref', 'me_range', 'qpmax', \n",
    "                      'aq-mode','trellis','fast_pskip', 'chroma_qp_offset', 'bframes', 'b_pyramid', \n",
    "                      'b_adapt', 'direct', 'ref', 'deblock', 'weightb', 'open_gop', 'weightp', \n",
    "                      'scenecut']\n",
    "\n",
    "# the list of measurements\n",
    "listVideo = []\n",
    "\n",
    "# we add each dataset in the list, converting the time to the right format\n",
    "# third line asserts that the measures are complete\n",
    "for v in v_names_train:\n",
    "    data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "    data['etime'] = [*map(elapsedtime_to_sec, data['elapsedtime'])]\n",
    "    assert data.shape == (201,34), v\n",
    "    inter = pd.get_dummies(data[to_dummy_features])\n",
    "    inter[predDimension] = data[predDimension]\n",
    "    listVideo.append(inter)\n",
    "\n",
    "cols = inter.columns\n",
    "cols = cols[:len(cols)-1]\n",
    "cols\n",
    "\n",
    "final = pd.concat(listVideo, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE Linear Reg 261017829.0\n",
      "Average MSE Decision Tree 260758459.0\n",
      "Average MSE Random Forest 260760034.0\n",
      "Average MSE Boosting Tree 260755085.0\n",
      "Average MSE Support Vector Regressor 302107392.0\n",
      "Average MSE Neural Network 261322284.0\n"
     ]
    }
   ],
   "source": [
    "#### Training set/test set of configurations\n",
    "X = final[cols]\n",
    "y = final[predDimension]\n",
    "\n",
    "# train - test separation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)\n",
    "\n",
    "# Linear regression\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)\n",
    "ypred_lin = lin.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeRegressor()\n",
    "dt.fit(X_train, y_train)\n",
    "ypred_dt = dt.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "ypred_rf = rf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting trees\n",
    "bt = XGBRegressor()\n",
    "bt.fit(X_train, y_train)\n",
    "y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(45, input_dim=45))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10, activation='relu'))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5, activation='relu'))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(1))\n",
    "\n",
    "model_nn.compile(loss='MSE', optimizer='Adam')\n",
    "model_nn.fit(X_train, y_train, epochs=5, verbose = False)\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "print(\"Average MSE Linear Reg\", np.round(mean_squared_error(y_test, ypred_lin)))\n",
    "print(\"Average MSE Decision Tree\", np.round(mean_squared_error(y_test, ypred_dt)))\n",
    "print(\"Average MSE Random Forest\", np.round(mean_squared_error(y_test, ypred_rf)))\n",
    "print(\"Average MSE Boosting Tree\", np.round(mean_squared_error(y_test, y_pred_bt)))\n",
    "print(\"Average MSE Support Vector Regressor\", np.round(mean_squared_error(y_test, y_pred_svr)))\n",
    "print(\"Average MSE Neural Network\", np.round(mean_squared_error(y_test, y_pred_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm kept : Random Forest\n",
    "\n",
    "#### Hyperparameter optimisation\n",
    "\n",
    "It is a compromise between the different input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed: 10.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 15, 'min_samples_leaf': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#### Training set/test set of configurations\n",
    "X = final[cols]\n",
    "y = final[predDimension]\n",
    "\n",
    "# train - test separation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)\n",
    "\n",
    "LA_rf = RandomForestRegressor()\n",
    "\n",
    "grid_search_larf = GridSearchCV(estimator = LA_rf,\n",
    "                                param_grid = {'n_estimators': [10, 50, 100],\n",
    "                                              # we didn't include 1 for min_samples_leaf to avoid overfitting\n",
    "                                         'min_samples_leaf' : [2, 5, 10],\n",
    "                                         'max_depth' : [3, 5, None],\n",
    "                                         'max_features' : [5, 15, 33]},\n",
    "                                scoring = 'neg_mean_squared_error',\n",
    "                                verbose = True,\n",
    "                                n_jobs = 5)\n",
    "\n",
    "grid_search_larf.fit(X_train, y_train)\n",
    "\n",
    "print(grid_search_larf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We count the number of occurences :\n",
    "\n",
    "{'max_depth': 5, 'max_features': 15, 'min_samples_leaf': 5, 'n_estimators': 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set of input videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_names_test = np.loadtxt(\"../../../results/raw_data/test_names.csv\", dtype= str)\n",
    "\n",
    "# the list of measurements\n",
    "listVideoTest = []\n",
    "\n",
    "# we add each dataset in the list, converting the time to the right format\n",
    "# third line asserts that the measures are complete\n",
    "for v in v_names_test:\n",
    "    data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "    data['etime'] = [*map(elapsedtime_to_sec, data['elapsedtime'])]\n",
    "    assert data.shape == (201,34), v\n",
    "    inter = pd.get_dummies(data[to_dummy_features])\n",
    "    inter[predDimension] = data[predDimension]\n",
    "    listVideoTest.append(inter)\n",
    "\n",
    "cols = inter.columns\n",
    "cols = cols[:len(cols)-1]\n",
    "cols\n",
    "\n",
    "final_test = pd.concat(listVideoTest, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_rf = RandomForestRegressor(max_depth = 5, \n",
    "                              max_features = 15, \n",
    "                              min_samples_leaf = 5, \n",
    "                              n_estimators = 100)\n",
    "\n",
    "X = final[cols]\n",
    "y = final[predDimension]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.5)\n",
    "\n",
    "X_te = final_test[cols]\n",
    "y_te = final_test[predDimension]\n",
    "\n",
    "LA_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LA_rf.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(y_pred) == len(v_names_test)*201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configs_BC = []\n",
    "\n",
    "for i in range(len(v_names_test)):\n",
    "    actual_video = [y_pred[i*201+k] for k in range(201)]\n",
    "    best_configs_BC.append(np.argmin(actual_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132,\n",
       " 132]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_configs_BC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"../../../results/raw_data/BC_results.csv\", best_configs_BC, fmt = '%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
