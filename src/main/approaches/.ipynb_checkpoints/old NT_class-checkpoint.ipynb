{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for arrays\n",
    "import numpy as np\n",
    "\n",
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# high-level plots\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# machine learning library\n",
    "# Principal Component Analysis - determine new axis for representing data\n",
    "from sklearn.decomposition import PCA\n",
    "# Random Forests -> vote between decision trees\n",
    "# Gradient boosting -> instead of a vote, upgrade the same tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "# To add interactions in linear regressions models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Elasticnet is an hybrid method between ridge and Lasso\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# To separate the data into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Simple clustering (iterative steps)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# we use it to interact with the file system\n",
    "import os\n",
    "# compute time\n",
    "from time import time\n",
    "\n",
    "# Neural network high level framework\n",
    "import keras\n",
    "# Sequential is a sequence of blocs\n",
    "# Input deals with the data fed to the network\n",
    "from keras.models import Sequential,Input,Model\n",
    "# Dense is a feedforward layer with fully connected nodes\n",
    "# Dropout allows to keep part of data, and to \"drop out\" a the rest\n",
    "# Flatten makes the data \"flat\", i.e. in one dimension\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "# Conv -> convolution, MaxPooling is relative to Pooling\n",
    "# Activation if the function composing the data in output of a layer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "\n",
    "\n",
    "from learner.mlearner import learn_with_interactions, learn_without_interactions, sample_random, stepwise_feature_selection\n",
    "from learner.model import genModelTermsfromString, Model, genModelfromCoeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NT:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # the data folder, see the markdown there for additional explanations\n",
    "        res_dir = \"../../../data/ugc/res_ugc/\"\n",
    "        \n",
    "        # the list of videos names, e.g. Animation_360P-3e40\n",
    "        # we sort the list so we keep the same ids between two launches\n",
    "        v_names = sorted(os.listdir(res_dir)) \n",
    "\n",
    "        # the list of measurements\n",
    "        listVideo = []\n",
    "\n",
    "        # we add each dataset in the list, converting the time to the right format\n",
    "        # third line asserts that the measures are complete\n",
    "        for v in sorted(v_names):\n",
    "            data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "            listVideo.append(data)\n",
    "        \n",
    "        self.listVideo = listVideo\n",
    "        \n",
    "        self.predDimension = \"kbs\"\n",
    "        \n",
    "        # to sample the source and the target using the same seed\n",
    "        self.random_state = np.random.randint(0,1000)\n",
    "        \n",
    "        self.features = ['cabac', '8x8dct', 'mbtree', 'rc_lookahead', 'analyse', 'me', 'subme', 'mixed_ref', 'me_range', \n",
    "                 'qpmax', 'aq-mode', 'trellis','fast_pskip', 'chroma_qp_offset', 'bframes', 'b_pyramid', \n",
    "                 'b_adapt', 'direct', 'ref', 'deblock', 'weightb', 'open_gop', 'weightp', 'scenecut']\n",
    "        \n",
    "        self.to_dummy_features = ['cabac', '8x8dct', 'mbtree', 'rc_lookahead', 'analyse', 'me', 'subme', 'mixed_ref', 'me_range', \n",
    "                 'qpmax', 'aq-mode', 'trellis','fast_pskip', 'chroma_qp_offset', 'bframes', 'b_pyramid', \n",
    "                 'b_adapt', 'direct', 'ref', 'deblock', 'weightb', 'open_gop', 'weightp', 'scenecut']\n",
    "    \n",
    "    \n",
    "    def mape(self, y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true-y_pred)/y_true))\n",
    "    \n",
    "    def learn(self, target_id, pct_train):\n",
    "\n",
    "        # random state , i.e. a seed to split the source and the target datasets\n",
    "        # by using the same set of configurations for training and testing\n",
    "        \n",
    "        # We define the target video, and split it into train-test\n",
    "        target = self.listVideo[target_id]\n",
    "        #print(target.shape)\n",
    "        dummies = pd.get_dummies(target[self.features], drop_first = False, columns=self.features)\n",
    "        X_tgt = pd.DataFrame(np.array(dummies, dtype=int))\n",
    "        #X_tgt = target[['cabac', 'mbtree','ref','subme']]\n",
    "        y_tgt = np.array(target[self.predDimension], dtype=float)\n",
    "        X_tgt_train, X_tgt_test, y_tgt_train, y_tgt_test = train_test_split(X_tgt, \n",
    "                                                                            y_tgt, \n",
    "                                                                            train_size=pct_train)\n",
    "        lf = RandomForestRegressor(n_estimators=100)\n",
    "        lf.fit(X_tgt_train, y_tgt_train)\n",
    "        y_tgt_pred_test = np.array(lf.predict(X_tgt_test)).reshape(-1,1)\n",
    "        #print(y_tgt_pred_test)\n",
    "\n",
    "        # We return the mean average percentage error \n",
    "        # between the real values of y_test from target \n",
    "        # and the predictions shifted \n",
    "        return self.mape(y_tgt_test, y_tgt_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.14749832459861906\n",
      "0.2 0.17886076587994545\n",
      "0.3 0.18425768743702525\n",
      "0.4 0.18093377724380116\n",
      "0.5 0.1732045768508626\n",
      "0.6 0.1776159748556177\n",
      "0.7 0.17588752935903565\n",
      "0.8 0.17780601959250406\n",
      "0.9 0.17775161673340825\n"
     ]
    }
   ],
   "source": [
    "nt = NT()\n",
    "\n",
    "for pct_train in np.round(np.arange(0.1, 1, 0.1),2):\n",
    "    print(pct_train, np.mean([nt.learn(target_id = 6, pct_train = pct_train) for i in range(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
