{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input-aware Learning (IaL)\n",
    "\n",
    "**Input-aware Learning (IaL)** was first designed to overcome the input sensitivity of programs when compiling them with PetaBricks. \n",
    "\n",
    "> See the reference @inproceedings{ding2015,\n",
    "  title={Autotuning algorithmic choice for input sensitivity},\n",
    "  author={Ding, Yufei and Ansel, Jason and Veeramachaneni, Kalyan and Shen, Xipeng and Oâ€™Reilly, Una-May and Amarasinghe, Saman},\n",
    "  booktitle={ACM SIGPLAN Notices},\n",
    "  volume={50},\n",
    "  number={6},\n",
    "  pages={379--390},\n",
    "  year={2015},\n",
    "  organization={ACM},\n",
    "  url={https://dl.acm.org/doi/pdf/10.1145/2813885.2737969},\n",
    "}\n",
    "\n",
    "Applied to the x264 case, it uses input properties of videos to propose a configuration working for a group of videos, sharing similar performances. \n",
    "\n",
    "\n",
    "According to Ding et al,  Input-Aware Learning can be broken down to six steps. \n",
    "\n",
    "\n",
    "Steps 1 to 4 are applied on the training set, while Step 5 and 6 consider a new input of the test set. \n",
    "\n",
    "**Step 1. Property extraction** - To mimic the domain knowledge of the expert, we use the videos' properties provided by the dataset of inputs\n",
    "\n",
    "**Step 2. Form groups of inputs** - \n",
    "Based on the dendogram of Figure 1, we report on videos' properties that can be used to characterize four performance groups :\n",
    "- Group 1. Action videos (high spatial and chunk complexities, Sports and News); \n",
    "- Group 2. Big resolution videos (low spatial and high temporal complexities, High Dynamic Range);\n",
    "- Group 3. Still image videos (low temporal and chunk complexities, Lectures and HowTo)\n",
    "- Group 4. Standard videos (average properties values, various contents)\n",
    "\n",
    "Similarly, we used the training set of videos to build four groups of inputs. \n",
    "\n",
    "**Step 3. Landmark creation** - For each group, we artificially build a video, being the centroid of all the input videos of its group. We then use this video to select a set of landmarks (i.e. configurations), potential candidates to optimize the performance for this group. \n",
    "\n",
    "**Step 4. Performance measurements** - For each input video, we save the performances of its landmarks (i.e. the landmarks kept in Step 3, corresponding to its group).\n",
    "\n",
    "**Step 5. Classify new inputs into a performance group** - Based on its input properties (see Step 1), we attribute a group to a new input video of the test set. It becomes a k-classification problem, k being the number of performance groups of Step 2. \n",
    "\n",
    "**Step 6. Propose a configuration for the new input** - We then propose a configuration based on the input properties of the video. It becomes a n-classification problem, where n is the number of landmarks kept for the group predicted in Step 5. We keep the best configuration predicted in Step 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for arrays\n",
    "import numpy as np\n",
    "\n",
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "# high-level plots\n",
    "import seaborn as sns\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# machine learning library\n",
    "# Principal Component Analysis - determine new axis for representing data\n",
    "from sklearn.decomposition import PCA\n",
    "# Random Forests -> vote between decision trees\n",
    "# Gradient boosting -> instead of a vote, upgrade the same tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "# To add interactions in linear regressions models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Elasticnet is an hybrid method between ridge and Lasso\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# To separate the data into training and test\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Simple clustering (iterative steps)\n",
    "from sklearn.cluster import KMeans\n",
    "# Support vector machine - support vector regressor\n",
    "from sklearn.svm import SVR, SVC\n",
    "# decision trees\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree, DecisionTreeClassifier\n",
    "# mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# gradient boosting trees\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# we use it to interact with the file system\n",
    "import os\n",
    "# compute time\n",
    "from time import time\n",
    "\n",
    "# Neural network high level framework\n",
    "import keras\n",
    "# Sequential is a sequence of blocs\n",
    "# Input deals with the data fed to the network\n",
    "from keras.models import Sequential,Input,Model\n",
    "# Dense is a feedforward layer with fully connected nodes\n",
    "# Dropout allows to keep part of data, and to \"drop out\" a the rest\n",
    "# Flatten makes the data \"flat\", i.e. in one dimension\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "# Conv -> convolution, MaxPooling is relative to Pooling\n",
    "# Activation if the function composing the data in output of a layer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set of input videos - Join all the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subme', 'mixed_ref', 'me_range', 'qpmax', 'aq-mode', 'trellis',\n",
       "       'fast_pskip', 'chroma_qp_offset', 'bframes', 'ref', 'weightp',\n",
       "       'rc_lookahead_10', 'rc_lookahead_20', 'rc_lookahead_30',\n",
       "       'rc_lookahead_40', 'rc_lookahead_50', 'rc_lookahead_60',\n",
       "       'rc_lookahead_None', 'analyse_0:0', 'analyse_0x113:0x113',\n",
       "       'analyse_0x3:0x113', 'analyse_0x3:0x133', 'analyse_0x3:0x3', 'me_dia',\n",
       "       'me_hex', 'me_tesa', 'me_umh', 'b_pyramid_1', 'b_pyramid_2',\n",
       "       'b_pyramid_None', 'b_adapt_1', 'b_adapt_2', 'b_adapt_None',\n",
       "       'direct_None', 'direct_auto', 'direct_spatial', 'deblock_0:0:0',\n",
       "       'deblock_1:0:0', 'weightb_1', 'weightb_None', 'open_gop_0',\n",
       "       'open_gop_None', 'scenecut_0', 'scenecut_40', 'scenecut_None'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_names_train = np.loadtxt(\"../../../results/raw_data/train_names.csv\", dtype= str)\n",
    "v_names_test = np.loadtxt(\"../../../results/raw_data/test_names.csv\", dtype= str)\n",
    "\n",
    "predDimension = 'kbs'\n",
    "\n",
    "#because x264 output is \"m:s\", where m is the number of minutes and s the number of seconds \n",
    "# we define a function to convert this format into the number of seconds\n",
    "def elapsedtime_to_sec(el):\n",
    "    tab = el.split(\":\")\n",
    "    return float(tab[0])*60+float(tab[1])\n",
    "\n",
    "# the data folder, see the markdown there for additional explanations\n",
    "res_dir = \"../../../data/ugc/res_ugc/\"\n",
    "\n",
    "# the list of videos names, e.g. Animation_360P-3e40\n",
    "# we sort the list so we keep the same ids between two launches\n",
    "v_names = sorted(os.listdir(res_dir)) \n",
    "\n",
    "to_dummy_features = [ 'rc_lookahead', 'analyse', 'me', 'subme', 'mixed_ref', 'me_range', 'qpmax', \n",
    "                      'aq-mode','trellis','fast_pskip', 'chroma_qp_offset', 'bframes', 'b_pyramid', \n",
    "                      'b_adapt', 'direct', 'ref', 'deblock', 'weightb', 'open_gop', 'weightp', \n",
    "                      'scenecut']\n",
    "\n",
    "# the list of measurements\n",
    "listVideo = []\n",
    "\n",
    "# we add each dataset in the list, converting the time to the right format\n",
    "# third line asserts that the measures are complete\n",
    "for v in v_names_train:\n",
    "    data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "    data['etime'] = [*map(elapsedtime_to_sec, data['elapsedtime'])]\n",
    "    assert data.shape == (201,34), v\n",
    "    inter = pd.get_dummies(data[to_dummy_features])\n",
    "    inter[predDimension] = data[predDimension]\n",
    "    listVideo.append(inter)\n",
    "\n",
    "cols = inter.columns\n",
    "cols = cols[:len(cols)-1]\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify inputs in groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perf_group</th>\n",
       "      <th>SLEEQ_DMOS</th>\n",
       "      <th>BANDING_DMOS</th>\n",
       "      <th>WIDTH</th>\n",
       "      <th>HEIGHT</th>\n",
       "      <th>SPATIAL_COMPLEXITY</th>\n",
       "      <th>TEMPORAL_COMPLEXITY</th>\n",
       "      <th>CHUNK_COMPLEXITY_VARIATION</th>\n",
       "      <th>COLOR_COMPLEXITY</th>\n",
       "      <th>video_category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FILENAME</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Animation_1080P-01b3</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.678859</td>\n",
       "      <td>4.653015</td>\n",
       "      <td>0.383054</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>-1.475487</td>\n",
       "      <td>-1.547345</td>\n",
       "      <td>-0.892454</td>\n",
       "      <td>-1.210798</td>\n",
       "      <td>-1.618194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animation_1080P-05f8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.844509</td>\n",
       "      <td>0.741729</td>\n",
       "      <td>0.383054</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>-0.147257</td>\n",
       "      <td>0.444086</td>\n",
       "      <td>2.545710</td>\n",
       "      <td>2.207516</td>\n",
       "      <td>-1.618194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animation_1080P-0c4f</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.655778</td>\n",
       "      <td>-0.377464</td>\n",
       "      <td>0.383054</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>0.422320</td>\n",
       "      <td>-0.963192</td>\n",
       "      <td>1.054868</td>\n",
       "      <td>-1.232460</td>\n",
       "      <td>-1.618194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animation_1080P-0cdf</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.294170</td>\n",
       "      <td>-0.059377</td>\n",
       "      <td>0.383054</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>-0.028644</td>\n",
       "      <td>0.430810</td>\n",
       "      <td>-0.103261</td>\n",
       "      <td>-0.448284</td>\n",
       "      <td>-1.618194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Animation_1080P-18f5</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.478821</td>\n",
       "      <td>-0.377464</td>\n",
       "      <td>0.383054</td>\n",
       "      <td>0.332504</td>\n",
       "      <td>1.289017</td>\n",
       "      <td>-0.958767</td>\n",
       "      <td>-0.051295</td>\n",
       "      <td>0.192920</td>\n",
       "      <td>-1.618194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vlog_720P-561e</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.678859</td>\n",
       "      <td>-0.377464</td>\n",
       "      <td>-0.239786</td>\n",
       "      <td>-0.333314</td>\n",
       "      <td>0.978979</td>\n",
       "      <td>-1.414583</td>\n",
       "      <td>-0.652893</td>\n",
       "      <td>0.457201</td>\n",
       "      <td>1.494379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vlog_720P-5d08</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.678859</td>\n",
       "      <td>-0.377464</td>\n",
       "      <td>-0.773092</td>\n",
       "      <td>-0.333314</td>\n",
       "      <td>3.257287</td>\n",
       "      <td>-0.303807</td>\n",
       "      <td>-0.437698</td>\n",
       "      <td>-0.158009</td>\n",
       "      <td>1.494379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vlog_720P-60f8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.444433</td>\n",
       "      <td>0.623920</td>\n",
       "      <td>-0.239786</td>\n",
       "      <td>-0.333314</td>\n",
       "      <td>0.234418</td>\n",
       "      <td>-0.042708</td>\n",
       "      <td>-0.364385</td>\n",
       "      <td>-0.149344</td>\n",
       "      <td>1.494379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vlog_720P-6410</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.455739</td>\n",
       "      <td>3.769441</td>\n",
       "      <td>-0.239786</td>\n",
       "      <td>-0.333314</td>\n",
       "      <td>-0.770856</td>\n",
       "      <td>2.121314</td>\n",
       "      <td>1.971065</td>\n",
       "      <td>-0.240326</td>\n",
       "      <td>1.494379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vlog_720P-6d56</th>\n",
       "      <td>3</td>\n",
       "      <td>0.629083</td>\n",
       "      <td>-0.353902</td>\n",
       "      <td>-0.239786</td>\n",
       "      <td>-0.333314</td>\n",
       "      <td>-0.329287</td>\n",
       "      <td>0.329026</td>\n",
       "      <td>1.646979</td>\n",
       "      <td>0.565512</td>\n",
       "      <td>1.494379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1397 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      perf_group  SLEEQ_DMOS  BANDING_DMOS     WIDTH  \\\n",
       "FILENAME                                                               \n",
       "Animation_1080P-01b3           2   -0.678859      4.653015  0.383054   \n",
       "Animation_1080P-05f8           0    0.844509      0.741729  0.383054   \n",
       "Animation_1080P-0c4f           3   -0.655778     -0.377464  0.383054   \n",
       "Animation_1080P-0cdf           0   -0.294170     -0.059377  0.383054   \n",
       "Animation_1080P-18f5           0   -0.478821     -0.377464  0.383054   \n",
       "...                          ...         ...           ...       ...   \n",
       "Vlog_720P-561e                 3   -0.678859     -0.377464 -0.239786   \n",
       "Vlog_720P-5d08                 0   -0.678859     -0.377464 -0.773092   \n",
       "Vlog_720P-60f8                 0    0.444433      0.623920 -0.239786   \n",
       "Vlog_720P-6410                 1   -0.455739      3.769441 -0.239786   \n",
       "Vlog_720P-6d56                 3    0.629083     -0.353902 -0.239786   \n",
       "\n",
       "                        HEIGHT  SPATIAL_COMPLEXITY  TEMPORAL_COMPLEXITY  \\\n",
       "FILENAME                                                                  \n",
       "Animation_1080P-01b3  0.332504           -1.475487            -1.547345   \n",
       "Animation_1080P-05f8  0.332504           -0.147257             0.444086   \n",
       "Animation_1080P-0c4f  0.332504            0.422320            -0.963192   \n",
       "Animation_1080P-0cdf  0.332504           -0.028644             0.430810   \n",
       "Animation_1080P-18f5  0.332504            1.289017            -0.958767   \n",
       "...                        ...                 ...                  ...   \n",
       "Vlog_720P-561e       -0.333314            0.978979            -1.414583   \n",
       "Vlog_720P-5d08       -0.333314            3.257287            -0.303807   \n",
       "Vlog_720P-60f8       -0.333314            0.234418            -0.042708   \n",
       "Vlog_720P-6410       -0.333314           -0.770856             2.121314   \n",
       "Vlog_720P-6d56       -0.333314           -0.329287             0.329026   \n",
       "\n",
       "                      CHUNK_COMPLEXITY_VARIATION  COLOR_COMPLEXITY  \\\n",
       "FILENAME                                                             \n",
       "Animation_1080P-01b3                   -0.892454         -1.210798   \n",
       "Animation_1080P-05f8                    2.545710          2.207516   \n",
       "Animation_1080P-0c4f                    1.054868         -1.232460   \n",
       "Animation_1080P-0cdf                   -0.103261         -0.448284   \n",
       "Animation_1080P-18f5                   -0.051295          0.192920   \n",
       "...                                          ...               ...   \n",
       "Vlog_720P-561e                         -0.652893          0.457201   \n",
       "Vlog_720P-5d08                         -0.437698         -0.158009   \n",
       "Vlog_720P-60f8                         -0.364385         -0.149344   \n",
       "Vlog_720P-6410                          1.971065         -0.240326   \n",
       "Vlog_720P-6d56                          1.646979          0.565512   \n",
       "\n",
       "                      video_category  \n",
       "FILENAME                              \n",
       "Animation_1080P-01b3       -1.618194  \n",
       "Animation_1080P-05f8       -1.618194  \n",
       "Animation_1080P-0c4f       -1.618194  \n",
       "Animation_1080P-0cdf       -1.618194  \n",
       "Animation_1080P-18f5       -1.618194  \n",
       "...                              ...  \n",
       "Vlog_720P-561e              1.494379  \n",
       "Vlog_720P-5d08              1.494379  \n",
       "Vlog_720P-60f8              1.494379  \n",
       "Vlog_720P-6410              1.494379  \n",
       "Vlog_720P-6d56              1.494379  \n",
       "\n",
       "[1397 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we load the file (in itself an aggregation of datasets)\n",
    "# the file is available in the data folder, then ugc_meta\n",
    "# each line is a video, and the columns are the different metrics\n",
    "# provided by Wang et. al.\n",
    "meta = pd.read_csv(\"../../../data/ugc/ugc_meta/all_features.csv\").set_index('FILENAME')\n",
    "# category is a high-level characterization of the content of the video\n",
    "# for an example, Sports for a sports video\n",
    "# you can see more details about different categories \n",
    "# and metrics per category in the resources/categories.csv file\n",
    "# I also recommand to read the Youtube UGC paper to understand why we consider these categories\n",
    "meta['category']=[str(meta.index[i]).split('_')[0] for i in range(meta.shape[0])]\n",
    "# a lot of NA, not a big feature importance, seems complicated to compute -> remove NOISE DMOS\n",
    "del meta['NOISE_DMOS']\n",
    "# fill NA with zeros\n",
    "meta = meta.fillna(0)\n",
    "# create a numeric variable (quanti) to compute the category\n",
    "# one video has one and only one category (1 to 1 in sql, so we can join the tables)\n",
    "# again, to do it properly, we should use dummies\n",
    "# but then we cannot compare directly the importances of the metrics to categories \n",
    "cat_tab = pd.Series(meta['category'].values).unique()\n",
    "meta['video_category'] = [np.where(cat_tab==meta['category'][i])[0][0] for i in range(len(meta['category']))]\n",
    "# delete the old columns (quali)\n",
    "del meta['category']\n",
    "# we normalize the variables, since height mean is about 1000, and complexity about 2\n",
    "# different scales do not behave correctly with learning algorithms\n",
    "for col in meta.columns:#[:len(meta.columns)-1]:\n",
    "    inter = np.array(meta[col],float)\n",
    "    meta[col] = (inter-np.mean(inter))/np.std(inter)\n",
    "# left join performance groups to the dataset of metrics\n",
    "perf = pd.read_csv(\"../../../results/raw_data/truth_group.csv\").set_index('FILENAME')\n",
    "meta_perf= perf.join(meta)\n",
    "# print the results for the training inputs\n",
    "meta_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm to classify videos in groups\n",
    "\n",
    "data : input properties\n",
    "\n",
    "predicted : group of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Decision Tree: 0.5428571428571428\n",
      "Test accuracy for Random Forest: 0.6547619047619048\n",
      "Test accuracy for Boosting Trees: 0.65\n",
      "Test accuracy for Support Vector: 0.6309523809523809\n",
      "Test accuracy for Neural Network: 0.5214285714285715\n"
     ]
    }
   ],
   "source": [
    "if 'str_video_cat' in meta_perf.columns:\n",
    "    del meta_perf['str_video_cat']\n",
    "\n",
    "X = np.array(meta_perf[[k for k in meta_perf.columns if k !='perf_group']], float)\n",
    "y = np.array(meta_perf['perf_group'], float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting trees\n",
    "bt = GradientBoostingClassifier()\n",
    "bt.fit(X_train, y_train)\n",
    "y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "# Support Vector Classifier\n",
    "svr = SVC()\n",
    "svr.fit(X_train, y_train)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X_train, pd.get_dummies(y_train), epochs=5, verbose = False)\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "conf = pd.crosstab(y_pred_dt, y_test)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Decision Tree: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_rf, y_test)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Random Forest: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_bt, y_test)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Boosting Trees: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_svr, y_test)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Support Vector: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(np.argmax(y_pred_nn, axis = 1), y_test)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Neural Network: '+ str(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm kept : Boosting Trees\n",
    "\n",
    "#### Hyperparameter optimization\n",
    "\n",
    "It is a compromise between the different input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_name_train = [v[:-4] for v in v_names_train]\n",
    "short_name_test = [v[:-4] for v in v_names_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training set/test set of configurations\n",
    "X_train = np.array(meta_perf.loc[short_name_train][[k for k in meta_perf.columns if k !='perf_group']], float)\n",
    "y_train = np.array(meta_perf.loc[short_name_train]['perf_group'], float)\n",
    "\n",
    "LA_gb = GradientBoostingClassifier()\n",
    "\n",
    "grid_search_larf = GridSearchCV(estimator = LA_gb,\n",
    "                                param_grid = {'min_samples_split': [5, 10, 20],\n",
    "                                              # we didn't include 1 for min_samples_leaf to avoid overfitting\n",
    "                                         'min_samples_leaf' : [2, 5, 10],\n",
    "                                         'max_depth' : [3, 5, None],\n",
    "                                         'max_features' : [5, 15, 33]},\n",
    "                                scoring = 'neg_mean_squared_error',\n",
    "                                verbose = True,\n",
    "                                n_jobs = 5)\n",
    "\n",
    "#grid_search_larf.fit(X_train, y_train)\n",
    "\n",
    "#print(grid_search_larf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "{'max_depth': 5, 'max_features': 5, 'min_samples_leaf': 10, 'min_samples_split': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7380952380952381\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0.0  1.0  2.0  3.0\n",
       "row_0                    \n",
       "0.0    114    0    2    9\n",
       "1.0      0   49    3    4\n",
       "2.0      0    4   72    4\n",
       "3.0      7    2    2   75"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(meta_perf.loc[short_name_test][[k for k in meta_perf.columns if k !='perf_group']], float)\n",
    "\n",
    "LA_gb = GradientBoostingClassifier(max_depth= 5,\n",
    "                                   max_features = 5, \n",
    "                                   min_samples_leaf = 10,\n",
    "                                   min_samples_split = 10)\n",
    "\n",
    "LA_gb.fit(X_train, y_train)\n",
    "\n",
    "pred_groups = LA_gb.predict(X_test)\n",
    "\n",
    "conf = pd.crosstab(pred_groups, np.array(meta_perf.loc[short_name_test]['perf_group'], float))\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print(\"Accuracy =\", str(val))\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy between training and test is a quite good result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 3, 2, 0, 3, 3, 2, 1, 3, 1, 0, 0, 0, 3, 3, 2, 0, 2, 0,\n",
       "       1, 0, 0, 1, 0, 2, 1, 3, 2, 3, 0, 3, 2, 0, 2, 2, 2, 3, 3, 2, 0, 1,\n",
       "       3, 0, 2, 3, 0, 0, 0, 0, 0, 2, 1, 3, 0, 2, 0, 2, 0, 0, 0, 2, 2, 3,\n",
       "       0, 2, 0, 3, 0, 0, 2, 3, 3, 0, 2, 2, 0, 2, 3, 3, 2, 2, 3, 0, 3, 3,\n",
       "       0, 0, 0, 2, 0, 1, 1, 0, 3, 3, 3, 3, 3, 0, 1, 2, 1, 0, 3, 2, 0, 3,\n",
       "       3, 1, 1, 3, 0, 0, 2, 2, 3, 2, 2, 3, 0, 2, 2, 0, 2, 2, 2, 3, 2, 2,\n",
       "       0, 2, 0, 3, 1, 0, 2, 1, 3, 0, 2, 0, 0, 1, 3, 3, 0, 1, 2, 0, 3, 1,\n",
       "       2, 3, 0, 0, 2, 3, 1, 3, 0, 0, 2, 1, 3, 1, 0, 0, 2, 2, 3, 2, 0, 3,\n",
       "       1, 1, 0, 0, 0, 2, 2, 0, 3, 3, 1, 3, 0, 0, 2, 1, 0, 0, 0, 3, 3, 2,\n",
       "       3, 0, 3, 3, 2, 0, 0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 3,\n",
       "       0, 0, 1, 1, 0, 3, 3, 0, 0, 1, 1, 3, 0, 3, 0, 2, 0, 0, 3, 2, 0, 2,\n",
       "       1, 3, 0, 1, 0, 0, 0, 0, 0, 3, 3, 2, 1, 0, 0, 3, 0, 1, 1, 0, 1, 2,\n",
       "       3, 3, 1, 2, 3, 3, 3, 0, 0, 0, 3, 0, 0, 2, 2, 0, 1, 1, 1, 2, 1, 1,\n",
       "       2, 3, 3, 2, 0, 2, 0, 0, 2, 2, 0, 2, 2, 2, 1, 1, 3, 2, 3, 3, 1, 0,\n",
       "       3, 1, 3, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 1, 3, 0, 1, 3, 2, 0, 0,\n",
       "       1, 2, 2, 3, 1, 2, 1, 0, 3, 3, 1, 1, 0, 1, 0, 1, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.savetxt(\"../../../results/raw_data/predicted_test_group_IaL.csv\", pred_groups, fmt='%i')\n",
    "grps_test = np.loadtxt(\"../../../results/raw_data/predicted_test_group_IaL.csv\", dtype = int)\n",
    "grps_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the landmarks\n",
    "\n",
    "For each group, we isolate the 5 configurations having the best performances.\n",
    "\n",
    "First, we compute the ratios of the bitrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video0</th>\n",
       "      <th>video1</th>\n",
       "      <th>video2</th>\n",
       "      <th>video3</th>\n",
       "      <th>video4</th>\n",
       "      <th>video5</th>\n",
       "      <th>video6</th>\n",
       "      <th>video7</th>\n",
       "      <th>video8</th>\n",
       "      <th>video9</th>\n",
       "      <th>...</th>\n",
       "      <th>video1040</th>\n",
       "      <th>video1041</th>\n",
       "      <th>video1042</th>\n",
       "      <th>video1043</th>\n",
       "      <th>video1044</th>\n",
       "      <th>video1045</th>\n",
       "      <th>video1046</th>\n",
       "      <th>video1047</th>\n",
       "      <th>video1048</th>\n",
       "      <th>video1049</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.862204</td>\n",
       "      <td>3.586389</td>\n",
       "      <td>2.567840</td>\n",
       "      <td>4.862221</td>\n",
       "      <td>3.169225</td>\n",
       "      <td>3.189802</td>\n",
       "      <td>3.412022</td>\n",
       "      <td>2.880442</td>\n",
       "      <td>3.824816</td>\n",
       "      <td>3.072162</td>\n",
       "      <td>...</td>\n",
       "      <td>7.321984</td>\n",
       "      <td>4.509879</td>\n",
       "      <td>3.410863</td>\n",
       "      <td>2.778554</td>\n",
       "      <td>3.141600</td>\n",
       "      <td>1.815321</td>\n",
       "      <td>1.960430</td>\n",
       "      <td>2.075508</td>\n",
       "      <td>1.744034</td>\n",
       "      <td>3.224831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.130092</td>\n",
       "      <td>1.933077</td>\n",
       "      <td>1.389767</td>\n",
       "      <td>1.172065</td>\n",
       "      <td>1.417829</td>\n",
       "      <td>1.302191</td>\n",
       "      <td>1.373230</td>\n",
       "      <td>1.170470</td>\n",
       "      <td>1.676993</td>\n",
       "      <td>1.538306</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500755</td>\n",
       "      <td>1.691921</td>\n",
       "      <td>1.265955</td>\n",
       "      <td>2.271980</td>\n",
       "      <td>1.447346</td>\n",
       "      <td>1.352919</td>\n",
       "      <td>1.627961</td>\n",
       "      <td>1.083952</td>\n",
       "      <td>1.122840</td>\n",
       "      <td>1.432093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.126783</td>\n",
       "      <td>1.136725</td>\n",
       "      <td>1.544691</td>\n",
       "      <td>1.420825</td>\n",
       "      <td>1.846335</td>\n",
       "      <td>1.434206</td>\n",
       "      <td>1.284300</td>\n",
       "      <td>1.577655</td>\n",
       "      <td>1.843583</td>\n",
       "      <td>1.071359</td>\n",
       "      <td>...</td>\n",
       "      <td>1.951180</td>\n",
       "      <td>2.122874</td>\n",
       "      <td>1.365082</td>\n",
       "      <td>1.225935</td>\n",
       "      <td>2.035878</td>\n",
       "      <td>1.701721</td>\n",
       "      <td>1.967654</td>\n",
       "      <td>1.179261</td>\n",
       "      <td>1.307660</td>\n",
       "      <td>1.209609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.135252</td>\n",
       "      <td>1.255231</td>\n",
       "      <td>1.549854</td>\n",
       "      <td>1.417216</td>\n",
       "      <td>1.860384</td>\n",
       "      <td>1.441762</td>\n",
       "      <td>1.300931</td>\n",
       "      <td>1.593014</td>\n",
       "      <td>1.849610</td>\n",
       "      <td>1.115791</td>\n",
       "      <td>...</td>\n",
       "      <td>1.964765</td>\n",
       "      <td>2.142246</td>\n",
       "      <td>1.342774</td>\n",
       "      <td>1.404374</td>\n",
       "      <td>1.967706</td>\n",
       "      <td>1.694929</td>\n",
       "      <td>1.949904</td>\n",
       "      <td>1.179663</td>\n",
       "      <td>1.301094</td>\n",
       "      <td>1.225856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.193487</td>\n",
       "      <td>2.546688</td>\n",
       "      <td>1.141010</td>\n",
       "      <td>1.820536</td>\n",
       "      <td>1.130004</td>\n",
       "      <td>1.842751</td>\n",
       "      <td>1.839037</td>\n",
       "      <td>1.194368</td>\n",
       "      <td>1.432343</td>\n",
       "      <td>1.721692</td>\n",
       "      <td>...</td>\n",
       "      <td>1.571653</td>\n",
       "      <td>2.190037</td>\n",
       "      <td>1.746266</td>\n",
       "      <td>3.587797</td>\n",
       "      <td>1.093759</td>\n",
       "      <td>1.066703</td>\n",
       "      <td>1.094103</td>\n",
       "      <td>1.248850</td>\n",
       "      <td>1.136048</td>\n",
       "      <td>2.156670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.042759</td>\n",
       "      <td>1.312574</td>\n",
       "      <td>1.277822</td>\n",
       "      <td>1.098890</td>\n",
       "      <td>1.339164</td>\n",
       "      <td>1.148410</td>\n",
       "      <td>1.291479</td>\n",
       "      <td>1.034207</td>\n",
       "      <td>1.295245</td>\n",
       "      <td>1.301235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.344430</td>\n",
       "      <td>1.252478</td>\n",
       "      <td>1.119108</td>\n",
       "      <td>1.719691</td>\n",
       "      <td>1.414472</td>\n",
       "      <td>1.287799</td>\n",
       "      <td>1.518524</td>\n",
       "      <td>1.068775</td>\n",
       "      <td>1.078492</td>\n",
       "      <td>1.164970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1.063946</td>\n",
       "      <td>1.332809</td>\n",
       "      <td>1.300628</td>\n",
       "      <td>1.149238</td>\n",
       "      <td>1.343390</td>\n",
       "      <td>1.241268</td>\n",
       "      <td>1.303251</td>\n",
       "      <td>1.084247</td>\n",
       "      <td>1.452455</td>\n",
       "      <td>1.368212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340898</td>\n",
       "      <td>1.332315</td>\n",
       "      <td>1.207371</td>\n",
       "      <td>1.779311</td>\n",
       "      <td>1.422829</td>\n",
       "      <td>1.296252</td>\n",
       "      <td>1.565146</td>\n",
       "      <td>1.044772</td>\n",
       "      <td>1.087553</td>\n",
       "      <td>1.232681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.133806</td>\n",
       "      <td>1.963083</td>\n",
       "      <td>1.430254</td>\n",
       "      <td>1.149238</td>\n",
       "      <td>1.421048</td>\n",
       "      <td>1.271634</td>\n",
       "      <td>1.411676</td>\n",
       "      <td>1.203450</td>\n",
       "      <td>1.561483</td>\n",
       "      <td>1.597412</td>\n",
       "      <td>...</td>\n",
       "      <td>1.561765</td>\n",
       "      <td>1.706384</td>\n",
       "      <td>1.319690</td>\n",
       "      <td>2.469687</td>\n",
       "      <td>1.472009</td>\n",
       "      <td>1.346861</td>\n",
       "      <td>1.569729</td>\n",
       "      <td>1.119159</td>\n",
       "      <td>1.127905</td>\n",
       "      <td>1.553307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.531351</td>\n",
       "      <td>2.759685</td>\n",
       "      <td>1.372104</td>\n",
       "      <td>2.067581</td>\n",
       "      <td>1.297777</td>\n",
       "      <td>2.056266</td>\n",
       "      <td>2.186113</td>\n",
       "      <td>1.349877</td>\n",
       "      <td>1.663221</td>\n",
       "      <td>2.066693</td>\n",
       "      <td>...</td>\n",
       "      <td>1.856370</td>\n",
       "      <td>2.579451</td>\n",
       "      <td>2.022114</td>\n",
       "      <td>4.238554</td>\n",
       "      <td>1.242817</td>\n",
       "      <td>1.258331</td>\n",
       "      <td>1.397348</td>\n",
       "      <td>1.589516</td>\n",
       "      <td>1.409675</td>\n",
       "      <td>2.489680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1.010198</td>\n",
       "      <td>1.670513</td>\n",
       "      <td>1.398224</td>\n",
       "      <td>1.121718</td>\n",
       "      <td>1.360367</td>\n",
       "      <td>1.135152</td>\n",
       "      <td>1.329221</td>\n",
       "      <td>1.062336</td>\n",
       "      <td>1.637008</td>\n",
       "      <td>1.474221</td>\n",
       "      <td>...</td>\n",
       "      <td>1.527168</td>\n",
       "      <td>1.447871</td>\n",
       "      <td>1.247527</td>\n",
       "      <td>1.852915</td>\n",
       "      <td>1.349037</td>\n",
       "      <td>1.230864</td>\n",
       "      <td>1.477360</td>\n",
       "      <td>1.015365</td>\n",
       "      <td>1.054236</td>\n",
       "      <td>1.451752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows Ã— 1050 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video0    video1    video2    video3    video4    video5    video6  \\\n",
       "index                                                                         \n",
       "0      1.862204  3.586389  2.567840  4.862221  3.169225  3.189802  3.412022   \n",
       "1      1.130092  1.933077  1.389767  1.172065  1.417829  1.302191  1.373230   \n",
       "2      1.126783  1.136725  1.544691  1.420825  1.846335  1.434206  1.284300   \n",
       "3      1.135252  1.255231  1.549854  1.417216  1.860384  1.441762  1.300931   \n",
       "4      1.193487  2.546688  1.141010  1.820536  1.130004  1.842751  1.839037   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "196    1.042759  1.312574  1.277822  1.098890  1.339164  1.148410  1.291479   \n",
       "197    1.063946  1.332809  1.300628  1.149238  1.343390  1.241268  1.303251   \n",
       "198    1.133806  1.963083  1.430254  1.149238  1.421048  1.271634  1.411676   \n",
       "199    1.531351  2.759685  1.372104  2.067581  1.297777  2.056266  2.186113   \n",
       "200    1.010198  1.670513  1.398224  1.121718  1.360367  1.135152  1.329221   \n",
       "\n",
       "         video7    video8    video9  ...  video1040  video1041  video1042  \\\n",
       "index                                ...                                    \n",
       "0      2.880442  3.824816  3.072162  ...   7.321984   4.509879   3.410863   \n",
       "1      1.170470  1.676993  1.538306  ...   1.500755   1.691921   1.265955   \n",
       "2      1.577655  1.843583  1.071359  ...   1.951180   2.122874   1.365082   \n",
       "3      1.593014  1.849610  1.115791  ...   1.964765   2.142246   1.342774   \n",
       "4      1.194368  1.432343  1.721692  ...   1.571653   2.190037   1.746266   \n",
       "...         ...       ...       ...  ...        ...        ...        ...   \n",
       "196    1.034207  1.295245  1.301235  ...   1.344430   1.252478   1.119108   \n",
       "197    1.084247  1.452455  1.368212  ...   1.340898   1.332315   1.207371   \n",
       "198    1.203450  1.561483  1.597412  ...   1.561765   1.706384   1.319690   \n",
       "199    1.349877  1.663221  2.066693  ...   1.856370   2.579451   2.022114   \n",
       "200    1.062336  1.637008  1.474221  ...   1.527168   1.447871   1.247527   \n",
       "\n",
       "       video1043  video1044  video1045  video1046  video1047  video1048  \\\n",
       "index                                                                     \n",
       "0       2.778554   3.141600   1.815321   1.960430   2.075508   1.744034   \n",
       "1       2.271980   1.447346   1.352919   1.627961   1.083952   1.122840   \n",
       "2       1.225935   2.035878   1.701721   1.967654   1.179261   1.307660   \n",
       "3       1.404374   1.967706   1.694929   1.949904   1.179663   1.301094   \n",
       "4       3.587797   1.093759   1.066703   1.094103   1.248850   1.136048   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "196     1.719691   1.414472   1.287799   1.518524   1.068775   1.078492   \n",
       "197     1.779311   1.422829   1.296252   1.565146   1.044772   1.087553   \n",
       "198     2.469687   1.472009   1.346861   1.569729   1.119159   1.127905   \n",
       "199     4.238554   1.242817   1.258331   1.397348   1.589516   1.409675   \n",
       "200     1.852915   1.349037   1.230864   1.477360   1.015365   1.054236   \n",
       "\n",
       "       video1049  \n",
       "index             \n",
       "0       3.224831  \n",
       "1       1.432093  \n",
       "2       1.209609  \n",
       "3       1.225856  \n",
       "4       2.156670  \n",
       "...          ...  \n",
       "196     1.164970  \n",
       "197     1.232681  \n",
       "198     1.553307  \n",
       "199     2.489680  \n",
       "200     1.451752  \n",
       "\n",
       "[201 rows x 1050 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitrates = listVideo[0][predDimension]\n",
    "ratio_bitrates = pd.DataFrame({\"index\" : range(201), \n",
    "                               \"video0\" : bitrates/min(bitrates)}).set_index(\"index\")\n",
    "\n",
    "for i in np.arange(1,len(listVideo),1):\n",
    "    bitrates = listVideo[i][predDimension]/min(listVideo[i][predDimension])\n",
    "    ratio_bitrates[\"video\"+str(i)] = bitrates\n",
    "\n",
    "ratio_bitrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video3',\n",
       " 'video5',\n",
       " 'video6',\n",
       " 'video9',\n",
       " 'video10',\n",
       " 'video15',\n",
       " 'video27',\n",
       " 'video35',\n",
       " 'video39',\n",
       " 'video41',\n",
       " 'video53',\n",
       " 'video54',\n",
       " 'video56',\n",
       " 'video59',\n",
       " 'video66',\n",
       " 'video93',\n",
       " 'video95',\n",
       " 'video104',\n",
       " 'video105',\n",
       " 'video107',\n",
       " 'video112',\n",
       " 'video121',\n",
       " 'video123',\n",
       " 'video125',\n",
       " 'video129',\n",
       " 'video140',\n",
       " 'video142',\n",
       " 'video143',\n",
       " 'video148',\n",
       " 'video159',\n",
       " 'video161',\n",
       " 'video170',\n",
       " 'video172',\n",
       " 'video180',\n",
       " 'video185',\n",
       " 'video189',\n",
       " 'video191',\n",
       " 'video197',\n",
       " 'video198',\n",
       " 'video202',\n",
       " 'video203',\n",
       " 'video206',\n",
       " 'video208',\n",
       " 'video213',\n",
       " 'video216',\n",
       " 'video219',\n",
       " 'video223',\n",
       " 'video228',\n",
       " 'video230',\n",
       " 'video231',\n",
       " 'video234',\n",
       " 'video252',\n",
       " 'video255',\n",
       " 'video266',\n",
       " 'video268',\n",
       " 'video272',\n",
       " 'video274',\n",
       " 'video276',\n",
       " 'video280',\n",
       " 'video281',\n",
       " 'video288',\n",
       " 'video296',\n",
       " 'video297',\n",
       " 'video299',\n",
       " 'video305',\n",
       " 'video313',\n",
       " 'video317',\n",
       " 'video327',\n",
       " 'video328',\n",
       " 'video332',\n",
       " 'video355',\n",
       " 'video358',\n",
       " 'video366',\n",
       " 'video370',\n",
       " 'video375',\n",
       " 'video379',\n",
       " 'video380',\n",
       " 'video384',\n",
       " 'video389',\n",
       " 'video406',\n",
       " 'video407',\n",
       " 'video408',\n",
       " 'video410',\n",
       " 'video412',\n",
       " 'video416',\n",
       " 'video418',\n",
       " 'video424',\n",
       " 'video425',\n",
       " 'video432',\n",
       " 'video433',\n",
       " 'video434',\n",
       " 'video439',\n",
       " 'video449',\n",
       " 'video458',\n",
       " 'video461',\n",
       " 'video462',\n",
       " 'video463',\n",
       " 'video467',\n",
       " 'video472',\n",
       " 'video475',\n",
       " 'video478',\n",
       " 'video485',\n",
       " 'video486',\n",
       " 'video491',\n",
       " 'video493',\n",
       " 'video495',\n",
       " 'video498',\n",
       " 'video508',\n",
       " 'video513',\n",
       " 'video514',\n",
       " 'video516',\n",
       " 'video529',\n",
       " 'video532',\n",
       " 'video533',\n",
       " 'video535',\n",
       " 'video540',\n",
       " 'video541',\n",
       " 'video542',\n",
       " 'video550',\n",
       " 'video555',\n",
       " 'video556',\n",
       " 'video563',\n",
       " 'video566',\n",
       " 'video594',\n",
       " 'video595',\n",
       " 'video600',\n",
       " 'video607',\n",
       " 'video610',\n",
       " 'video617',\n",
       " 'video624',\n",
       " 'video626',\n",
       " 'video634',\n",
       " 'video642',\n",
       " 'video643',\n",
       " 'video644',\n",
       " 'video647',\n",
       " 'video649',\n",
       " 'video661',\n",
       " 'video670',\n",
       " 'video678',\n",
       " 'video680',\n",
       " 'video684',\n",
       " 'video686',\n",
       " 'video689',\n",
       " 'video692',\n",
       " 'video703',\n",
       " 'video706',\n",
       " 'video707',\n",
       " 'video708',\n",
       " 'video709',\n",
       " 'video727',\n",
       " 'video736',\n",
       " 'video737',\n",
       " 'video739',\n",
       " 'video747',\n",
       " 'video749',\n",
       " 'video752',\n",
       " 'video753',\n",
       " 'video754',\n",
       " 'video756',\n",
       " 'video764',\n",
       " 'video765',\n",
       " 'video766',\n",
       " 'video791',\n",
       " 'video807',\n",
       " 'video808',\n",
       " 'video813',\n",
       " 'video815',\n",
       " 'video820',\n",
       " 'video829',\n",
       " 'video834',\n",
       " 'video838',\n",
       " 'video839',\n",
       " 'video841',\n",
       " 'video842',\n",
       " 'video843',\n",
       " 'video850',\n",
       " 'video852',\n",
       " 'video853',\n",
       " 'video860',\n",
       " 'video862',\n",
       " 'video865',\n",
       " 'video871',\n",
       " 'video877',\n",
       " 'video879',\n",
       " 'video891',\n",
       " 'video895',\n",
       " 'video898',\n",
       " 'video903',\n",
       " 'video913',\n",
       " 'video928',\n",
       " 'video935',\n",
       " 'video941',\n",
       " 'video953',\n",
       " 'video954',\n",
       " 'video956',\n",
       " 'video958',\n",
       " 'video963',\n",
       " 'video965',\n",
       " 'video966',\n",
       " 'video969',\n",
       " 'video973',\n",
       " 'video977',\n",
       " 'video989',\n",
       " 'video998',\n",
       " 'video1002',\n",
       " 'video1004',\n",
       " 'video1010',\n",
       " 'video1030',\n",
       " 'video1032',\n",
       " 'video1034',\n",
       " 'video1042',\n",
       " 'video1049']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_video_group = [\"video\"+str(i)\n",
    "                      for i in range(len(v_names_train))\n",
    "                      if y_train[i] == 2]\n",
    "names_video_group\n",
    "#[np.argmin(rankings[n]) for n in names_video_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmarks for the group 0 : [171, 104, 60, 32, 161]\n",
      "Landmarks for the group 1 : [2, 164, 3, 175, 193]\n",
      "Landmarks for the group 2 : [169, 168, 170, 165, 123]\n",
      "Landmarks for the group 3 : [169, 170, 168, 165, 123]\n"
     ]
    }
   ],
   "source": [
    "def get_landmarks(id_group):\n",
    "    \n",
    "    names_video_group = [\"video\"+str(i) \n",
    "                      for i in range(len(v_names_train))\n",
    "                      if y_train[i] == id_group]\n",
    "    \n",
    "    ratio_sum = [np.sum([ratio_bitrates[names_video_group].loc[i]]) for i in range(200)]\n",
    "    \n",
    "    ranks = [(i, ratio_sum[i]) for i in range(len(ratio_sum))]\n",
    "    \n",
    "    ranks.sort(key=lambda tup: tup[1], reverse=False)\n",
    "    \n",
    "    return [r[0] for r in ranks][0:5]\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"Landmarks for the group\", str(i), \":\", get_landmarks(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see the different landmarks for the different groups : configuration 191 seems to be a good choice in general, but is not a landmark for group 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = pd.DataFrame({'group0': get_landmarks(0),\n",
    "              'group1': get_landmarks(1),\n",
    "              'group2': get_landmarks(2),\n",
    "              'group3': get_landmarks(3)})\n",
    "\n",
    "landmarks.to_csv(\"../../../results/raw_data/predicted_landmarks_IaL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group0</th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>group3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104</td>\n",
       "      <td>164</td>\n",
       "      <td>168</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>175</td>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161</td>\n",
       "      <td>193</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group0  group1  group2  group3\n",
       "0     171       2     169     169\n",
       "1     104     164     168     170\n",
       "2      60       3     170     168\n",
       "3      32     175     165     165\n",
       "4     161     193     123     123"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(id_group):\n",
    "    \n",
    "    names_video_group_id = [\"video\"+str(i) \n",
    "                      for i in range(len(v_names_train))\n",
    "                      if y_train[i] == id_group]\n",
    "    \n",
    "    names_video_group = [v_names_train[i][:-4]\n",
    "                      for i in range(len(v_names_train))\n",
    "                      if y_train[i] == id_group]\n",
    "    \n",
    "    X = meta.loc[names_video_group]\n",
    "    \n",
    "    l = landmarks['group'+str(id_group)]\n",
    "    \n",
    "    y = [l[np.argmin(rankings.loc[l][n])] for n in names_video_group_id]\n",
    "    \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(id_group):\n",
    "    \n",
    "    names_video_group_id = [i\n",
    "                      for i in range(len(v_names_test))\n",
    "                      if grps_test[i] == id_group]\n",
    "    \n",
    "    names_video_group = [v_names_test[i][:-4]\n",
    "                      for i in range(len(v_names_test))\n",
    "                      if grps_test[i] == id_group]\n",
    "    \n",
    "    X = meta.loc[names_video_group]\n",
    "    \n",
    "    return (X, names_video_group_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Decision Tree: 0.0761904761904762\n",
      "Test accuracy for Random Forest: 0.009523809523809525\n",
      "Test accuracy for Boosting Trees: 0.02857142857142857\n",
      "Test accuracy for Support Vector: 0.06666666666666667\n",
      "Test accuracy for Neural Network: 0.24761904761904763\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(0)\n",
    "\n",
    "X_train, X_test, y_tr, y_te = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_tr)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_tr)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting trees\n",
    "bt = GradientBoostingClassifier()\n",
    "bt.fit(X_train, y_tr)\n",
    "y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "# Support Vector Classifier\n",
    "svr = SVC()\n",
    "svr.fit(X_train, y_tr)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X_train, pd.get_dummies(y_tr), epochs=5, verbose = False)\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "conf = pd.crosstab(y_pred_dt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Decision Tree: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_rf, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Random Forest: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_bt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Boosting Trees: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_svr, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Support Vector: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(np.argmax(y_pred_nn, axis = 1), y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Neural Network: '+ str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for the group 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(0)\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X, pd.get_dummies(y), epochs=5, verbose = False)\n",
    "    \n",
    "l = landmarks['group0']\n",
    "\n",
    "res = get_data_test(0)\n",
    "\n",
    "pred_grp0 = [l[pred] for pred in np.argmax(model_nn.predict(res[0]), axis = 1)]\n",
    "indexgrp0 = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Decision Tree: 0.1\n",
      "Test accuracy for Random Forest: 0.014285714285714285\n",
      "Test accuracy for Boosting Trees: 0.16\n",
      "Test accuracy for Support Vector: 0.08\n",
      "Test accuracy for Neural Network: 0.04\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(1)\n",
    "\n",
    "X_train, X_test, y_tr, y_te = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_tr)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_tr)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting trees\n",
    "bt = GradientBoostingClassifier()\n",
    "bt.fit(X_train, y_tr)\n",
    "y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "# Support Vector Classifier\n",
    "svr = SVC()\n",
    "svr.fit(X_train, y_tr)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X_train, pd.get_dummies(y_tr), epochs=5, verbose = False)\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "conf = pd.crosstab(y_pred_dt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Decision Tree: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_rf, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Random Forest: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_bt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Boosting Trees: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_svr, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Support Vector: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(np.argmax(y_pred_nn, axis = 1), y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Neural Network: '+ str(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for the group 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(1)\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X, pd.get_dummies(y), epochs=5, verbose = False)\n",
    "    \n",
    "l = landmarks['group1']\n",
    "\n",
    "res = get_data_test(1)\n",
    "\n",
    "pred_grp1 = [l[pred] for pred in np.argmax(model_nn.predict(res[0]), axis = 1)]\n",
    "indexgrp1 = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Decision Tree: 0.34375\n",
      "Test accuracy for Random Forest: 0.08095238095238096\n",
      "Test accuracy for Boosting Trees: 0.4375\n",
      "Test accuracy for Support Vector: 0.984375\n",
      "Test accuracy for Neural Network: 0.046875\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(2)\n",
    "\n",
    "X_train, X_test, y_tr, y_te = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_tr)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_tr)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting trees\n",
    "bt = GradientBoostingClassifier()\n",
    "bt.fit(X_train, y_tr)\n",
    "y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "# Support Vector Classifier\n",
    "svr = SVC()\n",
    "svr.fit(X_train, y_tr)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X_train, pd.get_dummies(y_tr), epochs=5, verbose = False)\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "conf = pd.crosstab(y_pred_dt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Decision Tree: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_rf, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Random Forest: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_bt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Boosting Trees: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_svr, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Support Vector: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(np.argmax(y_pred_nn, axis = 1), y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Neural Network: '+ str(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for the group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(2)\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X, pd.get_dummies(y), epochs=5, verbose = False)\n",
    "    \n",
    "l = landmarks['group2']\n",
    "\n",
    "res = get_data_test(2)\n",
    "\n",
    "pred_grp2 = [l[pred] for pred in np.argmax(model_nn.predict(res[0]), axis = 1)]\n",
    "indexgrp2 = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Decision Tree: 0.25510204081632654\n",
      "Test accuracy for Random Forest: 0.1\n",
      "Test accuracy for Boosting Trees: 0.32653061224489793\n",
      "Test accuracy for Support Vector: 0.826530612244898\n",
      "Test accuracy for Neural Network: 0.32653061224489793\n"
     ]
    }
   ],
   "source": [
    "X, y = get_data(3)\n",
    "\n",
    "X_train, X_test, y_tr, y_te = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_tr)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_tr)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Gradient Boosting trees\n",
    "bt = GradientBoostingClassifier()\n",
    "bt.fit(X_train, y_tr)\n",
    "y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "# Support Vector Classifier\n",
    "svr = SVC()\n",
    "svr.fit(X_train, y_tr)\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X_train, pd.get_dummies(y_tr), epochs=5, verbose = False)\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "\n",
    "conf = pd.crosstab(y_pred_dt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Decision Tree: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_rf, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_test)\n",
    "print('Test accuracy for Random Forest: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_bt, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Boosting Trees: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(y_pred_svr, y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Support Vector: '+ str(val))\n",
    "\n",
    "conf = pd.crosstab(np.argmax(y_pred_nn, axis = 1), y_te)\n",
    "val = np.sum(np.diag(conf))/len(y_te)\n",
    "print('Test accuracy for Neural Network: '+ str(val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network for the third group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_data(3)\n",
    "\n",
    "# neural network\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(9, input_dim=9))\n",
    "# These 2 nodes are linked to the 10 nodes of the following layer\n",
    "# The next nodes will receive a weighted sum of these 2 values as input\n",
    "model_nn.add(Dense(10))\n",
    "# we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "# rectified Linear Unit = identity for positive and 0 for negative values\n",
    "model_nn.add(Dense(5))\n",
    "# Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "model_nn.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model_nn.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model_nn.fit(X, pd.get_dummies(y), epochs=5, verbose = False)\n",
    "    \n",
    "l = landmarks['group3']\n",
    "\n",
    "res = get_data_test(3)\n",
    "\n",
    "pred_grp3 = [l[pred] for pred in np.argmax(model_nn.predict(res[0]), axis = 1)]\n",
    "indexgrp3 = res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "IaL_results = []\n",
    "\n",
    "val0 = 0\n",
    "val1 = 0\n",
    "val2 = 0\n",
    "val3 = 0\n",
    "\n",
    "for i in range(len(v_names_test)):\n",
    "    if i in indexgrp0:\n",
    "        IaL_results.append(pred_grp0[val0])\n",
    "        val0+=1\n",
    "    if i in indexgrp1:\n",
    "        IaL_results.append(pred_grp1[val1])\n",
    "        val1+=1\n",
    "    if i in indexgrp2:\n",
    "        IaL_results.append(pred_grp2[val2])\n",
    "        val2+=1\n",
    "    if i in indexgrp3:\n",
    "        IaL_results.append(pred_grp3[val3])\n",
    "        val3+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(IaL_results) == len(v_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 60, 123, 32, 165, 169, 161, 165, 165, 169, 164, 165, 3, 32, 171, 171, 170, 170, 169, 60, 168, 32, 164, 60, 161, 3, 32, 170, 193, 165, 170, 169, 32, 165, 168, 32, 165, 123, 123, 170, 169, 168, 161, 164, 165, 32, 168, 165, 60, 161, 60, 161, 32, 169, 3, 165, 60, 165, 32, 169, 32, 161, 32, 168, 168, 170, 161, 169, 171, 123, 32, 32, 169, 123, 123, 161, 169, 169, 32, 168, 165, 123, 165, 169, 169, 32, 123, 169, 32, 161, 32, 169, 104, 175, 193, 171, 165, 170, 165, 170, 165, 161, 3, 123, 175, 161, 165, 169, 60, 165, 170, 3, 164, 123, 104, 32, 123, 165, 123, 168, 168, 170, 104, 170, 168, 161, 169, 169, 169, 123, 170, 123, 32, 168, 161, 165, 3, 104, 169, 3, 170, 161, 168, 32, 104, 3, 165, 123, 161, 3, 170, 161, 123, 175, 169, 170, 161, 161, 170, 170, 2, 170, 60, 32, 169, 175, 169, 164, 32, 32, 170, 165, 170, 168, 161, 165, 193, 164, 161, 104, 32, 168, 169, 60, 170, 165, 2, 169, 161, 161, 169, 164, 161, 161, 161, 165, 165, 169, 169, 32, 123, 170, 169, 32, 171, 170, 32, 161, 169, 161, 161, 104, 123, 32, 161, 32, 161, 32, 3, 170, 161, 32, 164, 3, 161, 169, 165, 161, 60, 193, 3, 123, 32, 165, 161, 123, 32, 104, 123, 169, 161, 169, 175, 169, 161, 2, 32, 161, 161, 161, 161, 123, 169, 169, 3, 171, 60, 165, 161, 193, 175, 32, 3, 168, 169, 170, 3, 168, 165, 169, 165, 171, 161, 161, 170, 60, 60, 169, 169, 161, 3, 175, 2, 169, 3, 2, 169, 170, 170, 168, 32, 169, 161, 32, 170, 169, 32, 123, 169, 170, 175, 3, 165, 168, 165, 123, 164, 60, 165, 164, 165, 171, 168, 60, 60, 170, 32, 170, 170, 161, 161, 161, 3, 170, 161, 164, 165, 169, 60, 104, 3, 165, 169, 170, 3, 169, 2, 32, 123, 165, 3, 3, 32, 3, 104, 164, 123]\n"
     ]
    }
   ],
   "source": [
    "print(IaL_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"../../../results/raw_data/IaL_results.csv\", IaL_results, fmt = '%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
