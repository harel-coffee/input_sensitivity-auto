{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Reuse (MR)\n",
    "\n",
    "\n",
    "We arbitrarily choose a first video, learn a performance model on it, and select the best configuration minimizing the performance for this video. This approach represents the error made by a model trained on a  source input (i.e., a  first video) and transposed to a target input (i.e., a second video, different from the first one), without considering the difference of content between the source and the target. In theory, it corresponds to a fixed configuration, optimized for the first video. We add Model Reuse as a witness approach to measure how we can improve the standard performance model.\n",
    "\n",
    "The **Model Reuse** selects a video of the training set, apply a model on it and keep a near-optimal configuration working for this video. Then, it applies this configuration to all inputs of the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# for arrays\n",
    "import numpy as np\n",
    "\n",
    "# for dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "# high-level plots\n",
    "import seaborn as sns\n",
    "\n",
    "# statistics\n",
    "import scipy.stats as sc\n",
    "# hierarchical clustering, clusters\n",
    "from scipy.cluster.hierarchy import linkage, cut_tree, leaves_list\n",
    "from scipy import stats\n",
    "# statistical tests\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# machine learning library\n",
    "# Principal Component Analysis - determine new axis for representing data\n",
    "from sklearn.decomposition import PCA\n",
    "# Random Forests -> vote between decision trees\n",
    "# Gradient boosting -> instead of a vote, upgrade the same tree\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "# To add interactions in linear regressions models\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Elasticnet is an hybrid method between ridge and Lasso\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "# To separate the data into training and test\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# Simple clustering (iterative steps)\n",
    "from sklearn.cluster import KMeans\n",
    "# Support vector machine - support vector regressor\n",
    "from sklearn.svm import SVR\n",
    "# decision trees\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "# mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# gradient boosting trees\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# we use it to interact with the file system\n",
    "import os\n",
    "# compute time\n",
    "from time import time\n",
    "\n",
    "# Neural network high level framework\n",
    "import keras\n",
    "# Sequential is a sequence of blocs\n",
    "# Input deals with the data fed to the network\n",
    "from keras.models import Sequential,Input,Model\n",
    "# Dense is a feedforward layer with fully connected nodes\n",
    "# Dropout allows to keep part of data, and to \"drop out\" a the rest\n",
    "# Flatten makes the data \"flat\", i.e. in one dimension\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "# Conv -> convolution, MaxPooling is relative to Pooling\n",
    "# Activation if the function composing the data in output of a layer\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set of input videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subme', 'mixed_ref', 'me_range', 'qpmax', 'aq-mode', 'trellis',\n",
       "       'fast_pskip', 'chroma_qp_offset', 'bframes', 'ref', 'weightp',\n",
       "       'rc_lookahead_10', 'rc_lookahead_20', 'rc_lookahead_30',\n",
       "       'rc_lookahead_40', 'rc_lookahead_50', 'rc_lookahead_60',\n",
       "       'rc_lookahead_None', 'analyse_0:0', 'analyse_0x113:0x113',\n",
       "       'analyse_0x3:0x113', 'analyse_0x3:0x133', 'analyse_0x3:0x3', 'me_dia',\n",
       "       'me_hex', 'me_tesa', 'me_umh', 'b_pyramid_1', 'b_pyramid_2',\n",
       "       'b_pyramid_None', 'b_adapt_1', 'b_adapt_2', 'b_adapt_None',\n",
       "       'direct_None', 'direct_auto', 'direct_spatial', 'deblock_0:0:0',\n",
       "       'deblock_1:0:0', 'weightb_1', 'weightb_None', 'open_gop_0',\n",
       "       'open_gop_None', 'scenecut_0', 'scenecut_40', 'scenecut_None'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_names_train = np.loadtxt(\"../../../results/raw_data/train_names.csv\", dtype= str)\n",
    "\n",
    "predDimension = 'kbs'\n",
    "\n",
    "#because x264 output is \"m:s\", where m is the number of minutes and s the number of seconds \n",
    "# we define a function to convert this format into the number of seconds\n",
    "def elapsedtime_to_sec(el):\n",
    "    tab = el.split(\":\")\n",
    "    return float(tab[0])*60+float(tab[1])\n",
    "\n",
    "# the data folder, see the markdown there for additional explanations\n",
    "res_dir = \"../../../data/ugc/res_ugc/\"\n",
    "\n",
    "# the list of videos names, e.g. Animation_360P-3e40\n",
    "# we sort the list so we keep the same ids between two launches\n",
    "v_names = sorted(os.listdir(res_dir)) \n",
    "\n",
    "to_dummy_features = [ 'rc_lookahead', 'analyse', 'me', 'subme', 'mixed_ref', 'me_range', 'qpmax', \n",
    "                      'aq-mode','trellis','fast_pskip', 'chroma_qp_offset', 'bframes', 'b_pyramid', \n",
    "                      'b_adapt', 'direct', 'ref', 'deblock', 'weightb', 'open_gop', 'weightp', \n",
    "                      'scenecut']\n",
    "\n",
    "# the list of measurements\n",
    "listVideo = []\n",
    "\n",
    "# we add each dataset in the list, converting the time to the right format\n",
    "# third line asserts that the measures are complete\n",
    "for v in v_names_train:\n",
    "    data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "    data['etime'] = [*map(elapsedtime_to_sec, data['elapsedtime'])]\n",
    "    assert data.shape == (201,34), v\n",
    "    inter = pd.get_dummies(data[to_dummy_features])\n",
    "    inter[predDimension] = data[predDimension]\n",
    "    listVideo.append(inter)\n",
    "\n",
    "cols = inter.columns\n",
    "cols = cols[:len(cols)-1]\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE Linear Reg 4.438184404858307e+25\n",
      "Average MSE Decision Tree 12701403.0\n",
      "Average MSE Random Forest 8200172.0\n",
      "Average MSE Boosting Tree 9011004.0\n",
      "Average MSE Support Vector Regressor 38002536.0\n",
      "Average MSE Neural Network 462307774.0\n"
     ]
    }
   ],
   "source": [
    "mse_lin = []\n",
    "mse_dt = []\n",
    "mse_rf = []\n",
    "mse_bt = []\n",
    "mse_svr = []\n",
    "mse_nn = []\n",
    "\n",
    "for i in range(len(v_names_train[0:50])):\n",
    "    \n",
    "    #### Training set/test set of configurations\n",
    "    X = listVideo[i][cols]\n",
    "    y = listVideo[i][predDimension]\n",
    "    \n",
    "    # train - test separation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)\n",
    "\n",
    "    # Linear regression\n",
    "    lin = LinearRegression()\n",
    "    lin.fit(X_train, y_train)\n",
    "    ypred_lin = lin.predict(X_test)\n",
    "\n",
    "    # Decision Tree\n",
    "    dt = DecisionTreeRegressor()\n",
    "    dt.fit(X_train, y_train)\n",
    "    ypred_dt = dt.predict(X_test)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train)\n",
    "    ypred_rf = rf.predict(X_test)\n",
    "\n",
    "    # Gradient Boosting trees\n",
    "    bt = XGBRegressor()\n",
    "    bt.fit(X_train, y_train)\n",
    "    y_pred_bt = bt.predict(X_test)\n",
    "\n",
    "    # Support Vector Regressor\n",
    "    svr = SVR()\n",
    "    svr.fit(X_train, y_train)\n",
    "    y_pred_svr = svr.predict(X_test)\n",
    "\n",
    "\n",
    "    # neural network\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(45, input_dim=45))\n",
    "    # These 2 nodes are linked to the 10 nodes of the following layer\n",
    "    # The next nodes will receive a weighted sum of these 2 values as input\n",
    "    model_nn.add(Dense(10, activation='relu'))\n",
    "    # we add an activation function to compose the result (i.e. the weighted sum) by reLU\n",
    "    # rectified Linear Unit = identity for positive and 0 for negative values\n",
    "    model_nn.add(Dense(5, activation='relu'))\n",
    "    # Finally, we aggregate the 5 last values in one layer of one value, our prediction :)\n",
    "    model_nn.add(Dense(1))\n",
    "\n",
    "    model_nn.compile(loss='MSE', optimizer='Adam')\n",
    "    model_nn.fit(X_train, y_train, epochs=5, verbose = False)\n",
    "    y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "    mse_lin.append(mean_squared_error(y_test, ypred_lin))\n",
    "    mse_dt.append(mean_squared_error(y_test, ypred_dt))\n",
    "    mse_rf.append(mean_squared_error(y_test, ypred_rf))\n",
    "    mse_bt.append(mean_squared_error(y_test, y_pred_bt))\n",
    "    mse_svr.append(mean_squared_error(y_test, y_pred_svr))\n",
    "    mse_nn.append(mean_squared_error(y_test, y_pred_nn))\n",
    "\n",
    "print(\"Average MSE Linear Reg\", np.round(np.mean(mse_lin)))\n",
    "print(\"Average MSE Decision Tree\", np.round(np.mean(mse_dt)))\n",
    "print(\"Average MSE Random Forest\", np.round(np.mean(mse_rf)))\n",
    "print(\"Average MSE Boosting Tree\", np.round(np.mean(mse_bt)))\n",
    "print(\"Average MSE Support Vector Regressor\", np.round(np.mean(mse_svr)))\n",
    "print(\"Average MSE Neural Network\", np.round(np.mean(mse_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Algorithm kept : Gradient Boosting Trres\n",
    "\n",
    "#### Hyperparameter optimisation\n",
    "\n",
    "It is a compromise between the different input videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  50 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=5)]: Done 396 out of 405 | elapsed:    8.6s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=5)]: Done 396 out of 405 | elapsed:    7.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=5)]: Done 396 out of 405 | elapsed:    7.6s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 10}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=5)]: Done 396 out of 405 | elapsed:    8.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 15, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=5)]: Done 396 out of 405 | elapsed:    8.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=5)]: Done 396 out of 405 | elapsed:    7.5s remaining:    0.2s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 15, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    8.2s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 110 tasks      | elapsed:    2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_features': 33, 'min_samples_leaf': 2, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done 405 out of 405 | elapsed:    7.8s finished\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    #### Training set/test set of configurations\n",
    "    X = listVideo[i][cols]\n",
    "    y = listVideo[i][predDimension]\n",
    "    \n",
    "    # train - test separation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7)\n",
    "    \n",
    "    LA_rf = RandomForestRegressor()\n",
    "\n",
    "    grid_search_larf = GridSearchCV(estimator = LA_rf,\n",
    "                                    param_grid = {'n_estimators': [10, 50, 100],\n",
    "                                                  # we didn't include 1 for min_samples_leaf to avoid overfitting\n",
    "                                             'min_samples_leaf' : [2, 5, 10],\n",
    "                                             'max_depth' : [3, 5, None],\n",
    "                                             'max_features' : [5, 15, 33]},\n",
    "                                    scoring = 'neg_mean_squared_error',\n",
    "                                    verbose = True,\n",
    "                                    n_jobs = 5)\n",
    "\n",
    "    grid_search_larf.fit(X_train, y_train)\n",
    "\n",
    "    print(grid_search_larf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We count the number of occurences :\n",
    "\n",
    "{'max_depth' : None, 'max_features' : 33, 'min_sample_leaf' : 2, 'n_estimators' : 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subme', 'mixed_ref', 'me_range', 'qpmax', 'aq-mode', 'trellis',\n",
       "       'fast_pskip', 'chroma_qp_offset', 'bframes', 'ref', 'weightp',\n",
       "       'rc_lookahead_10', 'rc_lookahead_20', 'rc_lookahead_30',\n",
       "       'rc_lookahead_40', 'rc_lookahead_50', 'rc_lookahead_60',\n",
       "       'rc_lookahead_None', 'analyse_0:0', 'analyse_0x113:0x113',\n",
       "       'analyse_0x3:0x113', 'analyse_0x3:0x133', 'analyse_0x3:0x3', 'me_dia',\n",
       "       'me_hex', 'me_tesa', 'me_umh', 'b_pyramid_1', 'b_pyramid_2',\n",
       "       'b_pyramid_None', 'b_adapt_1', 'b_adapt_2', 'b_adapt_None',\n",
       "       'direct_None', 'direct_auto', 'direct_spatial', 'deblock_0:0:0',\n",
       "       'deblock_1:0:0', 'weightb_1', 'weightb_None', 'open_gop_0',\n",
       "       'open_gop_None', 'scenecut_0', 'scenecut_40', 'scenecut_None'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_names_test = np.loadtxt(\"../../../results/raw_data/test_names.csv\", dtype= str)\n",
    "\n",
    "# the list of measurements\n",
    "listVideoTest = []\n",
    "\n",
    "# we add each dataset in the list, converting the time to the right format\n",
    "# third line asserts that the measures are complete\n",
    "for v in v_names_test:\n",
    "    data = pd.read_table(res_dir+v, delimiter = ',')\n",
    "    data['etime'] = [*map(elapsedtime_to_sec, data['elapsedtime'])]\n",
    "    assert data.shape == (201,34), v\n",
    "    inter = pd.get_dummies(data[to_dummy_features])\n",
    "    inter[predDimension] = data[predDimension]\n",
    "    listVideoTest.append(inter)\n",
    "\n",
    "cols = inter.columns\n",
    "cols = cols[:len(cols)-1]\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each input of the test set, we select arbitrarly an input of the training set, and train a model on it, then we keep this configuration for the test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config_MR = []\n",
    "\n",
    "for i in range(len(v_names_test)):\n",
    "    # arbitrarly chosen\n",
    "    index_train = 788\n",
    "    vid = listVideo[index_train]\n",
    "    X = vid[cols]\n",
    "    y = vid[predDimension]\n",
    "    \n",
    "    LA_rf = RandomForestRegressor(max_depth = None, \n",
    "                                  max_features = 33, \n",
    "                                  min_samples_leaf = 2, \n",
    "                                  n_estimators = 50)\n",
    "    LA_rf.fit(X, y)\n",
    "    y_pred = LA_rf.predict(X)\n",
    "    \n",
    "    best_config_MR.append(np.argmin(y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[175,\n",
       " 92,\n",
       " 175,\n",
       " 193,\n",
       " 92,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 164,\n",
       " 175,\n",
       " 175,\n",
       " 164,\n",
       " 92,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 164,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 92,\n",
       " 92,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 2,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 164,\n",
       " 193,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 92,\n",
       " 193,\n",
       " 175,\n",
       " 92,\n",
       " 193,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 164,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 92,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 164,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 193,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 92,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 193,\n",
       " 175,\n",
       " 92,\n",
       " 193,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 193,\n",
       " 175,\n",
       " 164,\n",
       " 92,\n",
       " 175,\n",
       " 175,\n",
       " 175,\n",
       " 164,\n",
       " 92,\n",
       " 175]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_config_MR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"../../../results/raw_data/MR_results.csv\", best_config_MR, fmt = '%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
